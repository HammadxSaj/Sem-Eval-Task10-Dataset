{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, Trainer, TrainingArguments, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:59:40.389819Z","iopub.execute_input":"2024-11-30T09:59:40.392298Z","iopub.status.idle":"2024-11-30T09:59:59.819477Z","shell.execute_reply.started":"2024-11-30T09:59:40.392235Z","shell.execute_reply":"2024-11-30T09:59:59.818503Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Download datasets\n!wget -P /kaggle/working -nc \"https://raw.githubusercontent.com/HammadxSaj/Sem-Eval-Task10-Dataset/refs/heads/main/final_cleaned_train.csv\"\n# !wget -P /kaggle/working -nc \"https://raw.githubusercontent.com/HammadxSaj/Sem-Eval-Task10-Dataset/refs/heads/main/final_combined_augmented.csv\"\n!wget -P /kaggle/working -nc \"https://raw.githubusercontent.com/HammadxSaj/Sem-Eval-Task10-Dataset/refs/heads/main/final_cleaned_validation.csv\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:59:59.821050Z","iopub.execute_input":"2024-11-30T09:59:59.821601Z","iopub.status.idle":"2024-11-30T10:00:02.315638Z","shell.execute_reply.started":"2024-11-30T09:59:59.821571Z","shell.execute_reply":"2024-11-30T10:00:02.314361Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"--2024-11-30 10:00:00--  https://raw.githubusercontent.com/HammadxSaj/Sem-Eval-Task10-Dataset/refs/heads/main/final_cleaned_train.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12659880 (12M) [text/plain]\nSaving to: '/kaggle/working/final_cleaned_train.csv'\n\nfinal_cleaned_train 100%[===================>]  12.07M  --.-KB/s    in 0.08s   \n\n2024-11-30 10:00:00 (160 MB/s) - '/kaggle/working/final_cleaned_train.csv' saved [12659880/12659880]\n\n--2024-11-30 10:00:02--  https://raw.githubusercontent.com/HammadxSaj/Sem-Eval-Task10-Dataset/refs/heads/main/final_cleaned_validation.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1251223 (1.2M) [text/plain]\nSaving to: '/kaggle/working/final_cleaned_validation.csv'\n\nfinal_cleaned_valid 100%[===================>]   1.19M  --.-KB/s    in 0.04s   \n\n2024-11-30 10:00:02 (31.1 MB/s) - '/kaggle/working/final_cleaned_validation.csv' saved [1251223/1251223]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load the training data\ndf = pd.read_csv('/kaggle/working/final_cleaned_train.csv')\n\n# Inspect the dataframe\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T10:00:02.317204Z","iopub.execute_input":"2024-11-30T10:00:02.317543Z","iopub.status.idle":"2024-11-30T10:00:02.523615Z","shell.execute_reply.started":"2024-11-30T10:00:02.317506Z","shell.execute_reply":"2024-11-30T10:00:02.522625Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   year  month  day country                             title  \\\n0  1994      1    7      us  Recall Notification: FSIS-024-94   \n1  1994      3   10      us  Recall Notification: FSIS-033-94   \n2  1994      3   28      us  Recall Notification: FSIS-014-94   \n3  1994      4    3      us  Recall Notification: FSIS-009-94   \n4  1994      7    1      us  Recall Notification: FSIS-001-94   \n\n                                                text hazard-category  \\\n0  Date Opened: Date Closed: Name: GERHARD'S NAPA...      biological   \n1  Date Opened: Date Closed: Name: WIMMER'S MEAT ...      biological   \n2  Date Opened: Date Closed: Name: WILLOW FOODS I...      biological   \n3  Date Opened: Date Closed: M Name: OSCAR MAYER ...  foreign bodies   \n4  Date Opened: Date Closed: Name: TYSON FOODS Im...  foreign bodies   \n\n               product-category                  hazard  \\\n0  meat, egg and dairy products  listeria monocytogenes   \n1  meat, egg and dairy products            listeria spp   \n2  meat, egg and dairy products  listeria monocytogenes   \n3  meat, egg and dairy products        plastic fragment   \n4  meat, egg and dairy products        plastic fragment   \n\n                       product  \n0               smoked sausage  \n1                      sausage  \n2                   ham slices  \n3  thermal processed pork meat  \n4               chicken breast  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>country</th>\n      <th>title</th>\n      <th>text</th>\n      <th>hazard-category</th>\n      <th>product-category</th>\n      <th>hazard</th>\n      <th>product</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1994</td>\n      <td>1</td>\n      <td>7</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-024-94</td>\n      <td>Date Opened: Date Closed: Name: GERHARD'S NAPA...</td>\n      <td>biological</td>\n      <td>meat, egg and dairy products</td>\n      <td>listeria monocytogenes</td>\n      <td>smoked sausage</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1994</td>\n      <td>3</td>\n      <td>10</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-033-94</td>\n      <td>Date Opened: Date Closed: Name: WIMMER'S MEAT ...</td>\n      <td>biological</td>\n      <td>meat, egg and dairy products</td>\n      <td>listeria spp</td>\n      <td>sausage</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1994</td>\n      <td>3</td>\n      <td>28</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-014-94</td>\n      <td>Date Opened: Date Closed: Name: WILLOW FOODS I...</td>\n      <td>biological</td>\n      <td>meat, egg and dairy products</td>\n      <td>listeria monocytogenes</td>\n      <td>ham slices</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1994</td>\n      <td>4</td>\n      <td>3</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-009-94</td>\n      <td>Date Opened: Date Closed: M Name: OSCAR MAYER ...</td>\n      <td>foreign bodies</td>\n      <td>meat, egg and dairy products</td>\n      <td>plastic fragment</td>\n      <td>thermal processed pork meat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1994</td>\n      <td>7</td>\n      <td>1</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-001-94</td>\n      <td>Date Opened: Date Closed: Name: TYSON FOODS Im...</td>\n      <td>foreign bodies</td>\n      <td>meat, egg and dairy products</td>\n      <td>plastic fragment</td>\n      <td>chicken breast</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Data preprocessing\n\n# Drop unnecessary columns for training\ndf = df[['text', 'hazard-category', 'product-category']]\n\n# Drop rows with missing values\ndf.dropna(inplace=True)\n\n# Initialize label encoders\nhazard_category_encoder = LabelEncoder()\nproduct_category_encoder = LabelEncoder()\n# hazard_encoder = LabelEncoder()\n# product_encoder = LabelEncoder()\n\n# Fit the encoders\nhazard_category_encoder.fit(df['hazard-category'])\nproduct_category_encoder.fit(df['product-category'])\n# hazard_encoder.fit(df['hazard'])\n# product_encoder.fit(df['product'])\n\n# Transform the labels\ndf['hazard-category'] = hazard_category_encoder.transform(df['hazard-category'])\ndf['product-category'] = product_category_encoder.transform(df['product-category'])\n# df['hazard'] = hazard_encoder.transform(df['hazard'])\n# df['product'] = product_encoder.transform(df['product'])\n\n# Split the data into train and validation sets\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(f\"Number of training samples: {len(train_df)}\")\nprint(f\"Number of validation samples: {len(val_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T10:00:02.525698Z","iopub.execute_input":"2024-11-30T10:00:02.526080Z","iopub.status.idle":"2024-11-30T10:00:02.549649Z","shell.execute_reply.started":"2024-11-30T10:00:02.526051Z","shell.execute_reply":"2024-11-30T10:00:02.548554Z"}},"outputs":[{"name":"stdout","text":"Number of training samples: 4772\nNumber of validation samples: 1194\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Define the FoodHazardDataset class\nclass FoodHazardDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, hazard_categories, product_categories):\n        self.encodings = encodings\n        self.hazard_categories = hazard_categories\n        self.product_categories = product_categories\n        # self.hazards = hazards\n        # self.products = products\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['hazard_category_labels'] = torch.tensor(self.hazard_categories[idx])\n        item['product_category_labels'] = torch.tensor(self.product_categories[idx])\n        # item['hazard_labels'] = torch.tensor(self.hazards[idx])\n        # item['product_labels'] = torch.tensor(self.products[idx])\n        return item\n\n    def __len__(self):\n        return len(self.hazard_categories)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T10:00:02.551035Z","iopub.execute_input":"2024-11-30T10:00:02.551441Z","iopub.status.idle":"2024-11-30T10:00:02.559257Z","shell.execute_reply.started":"2024-11-30T10:00:02.551396Z","shell.execute_reply":"2024-11-30T10:00:02.558456Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Define the number of unique labels for each category\nnum_hazard_category_labels = len(hazard_category_encoder.classes_)\nnum_product_category_labels = len(product_category_encoder.classes_)\n# num_hazard_labels = len(hazard_encoder.classes_)\n# num_product_labels = len(product_encoder.classes_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T10:00:02.560222Z","iopub.execute_input":"2024-11-30T10:00:02.560494Z","iopub.status.idle":"2024-11-30T10:00:02.568956Z","shell.execute_reply.started":"2024-11-30T10:00:02.560469Z","shell.execute_reply":"2024-11-30T10:00:02.568029Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoModel\nimport torch.nn as nn\n\nclass TransformerForFoodHazardClassification(nn.Module):\n    def __init__(self, model_name, num_labels_dict):\n        super().__init__()\n        self.transformer = AutoModel.from_pretrained(model_name)\n        # Uncomment the line below if you want to use dropout\n        # self.dropout = nn.Dropout(self.transformer.config.hidden_dropout_prob)\n\n        hidden_size = self.transformer.config.hidden_size\n\n        # Classifiers for the four labels\n        self.hazard_category_classifier = nn.Linear(hidden_size, num_labels_dict['hazard_category'])\n        self.product_category_classifier = nn.Linear(hidden_size, num_labels_dict['product_category'])\n        # self.hazard_classifier = nn.Linear(hidden_size, num_labels_dict['hazard'])\n        # self.product_classifier = nn.Linear(hidden_size, num_labels_dict['product'])\n\n        # Loss function\n        self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                hazard_category_labels=None, product_category_labels=None):\n        # Check if the model supports token_type_ids\n        if \"token_type_ids\" in self.transformer.forward.__code__.co_varnames:\n            outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        else:\n            # For DistilBERT and similar models that do not accept token_type_ids\n            outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n\n        # Select pooled output for models like BERT and DeBERTa, or use CLS token for others\n        if hasattr(outputs, 'pooler_output'):\n            pooled_output = outputs.pooler_output\n        else:\n            pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n\n        # Apply dropout if using\n        # pooled_output = self.dropout(pooled_output)\n\n        # Predict the four labels\n        hazard_category_logits = self.hazard_category_classifier(pooled_output)\n        product_category_logits = self.product_category_classifier(pooled_output)\n        # hazard_logits = self.hazard_classifier(pooled_output)\n        # product_logits = self.product_classifier(pooled_output)\n\n        loss = None\n        if hazard_category_labels is not None and product_category_labels is not None:\n            # Compute loss for each task\n            hazard_category_loss = self.loss_fct(hazard_category_logits, hazard_category_labels)\n            product_category_loss = self.loss_fct(product_category_logits, product_category_labels)\n            # hazard_loss = self.loss_fct(hazard_logits, hazard_labels)\n            # product_loss = self.loss_fct(product_logits, product_labels)\n\n            # Aggregate losses\n            loss = hazard_category_loss + product_category_loss\n\n        # Return the loss and logits\n        output = (hazard_category_logits, product_category_logits)\n        return ((loss,) + output) if loss is not None else output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T10:00:02.570337Z","iopub.execute_input":"2024-11-30T10:00:02.570687Z","iopub.status.idle":"2024-11-30T10:00:02.583002Z","shell.execute_reply.started":"2024-11-30T10:00:02.570660Z","shell.execute_reply":"2024-11-30T10:00:02.582167Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Define the compute_metrics function to calculate both accuracy and average F1 score across all labels\n\ndef compute_metrics(pred):\n\n    labels = pred.label_ids\n\n    preds = pred.predictions\n\n\n\n    # Unpack labels and predictions for each task\n\n    hazard_category_labels = labels[0]\n\n    product_category_labels = labels[1]\n\n    # hazard_labels = labels[2]\n\n    # product_labels = labels[3]\n\n\n\n    hazard_category_preds = preds[0].argmax(-1)\n\n    product_category_preds = preds[1].argmax(-1)\n\n    # hazard_preds = preds[2].argmax(-1)\n\n    # product_preds = preds[3].argmax(-1)\n\n\n\n    # Compute accuracy for each task (can be used separately if needed)\n\n    hazard_category_acc = accuracy_score(hazard_category_labels, hazard_category_preds)\n\n    product_category_acc = accuracy_score(product_category_labels, product_category_preds)\n\n    # hazard_acc = accuracy_score(hazard_labels, hazard_preds)\n\n    # product_acc = accuracy_score(product_labels, product_preds)\n\n\n\n    # Compute F1 score for each task\n\n    hazard_category_f1 = f1_score(hazard_category_labels, hazard_category_preds, average='weighted')\n\n    product_category_f1 = f1_score(product_category_labels, product_category_preds, average='weighted')\n\n    # hazard_f1 = f1_score(hazard_labels, hazard_preds, average='weighted')\n\n    # product_f1 = f1_score(product_labels, product_preds, average='weighted')\n\n\n\n    # Compute average F1 score across all tasks\n\n    avg_f1 = (hazard_category_f1 + product_category_f1) / 4\n\n\n\n    # Optionally, you can also compute average accuracy across tasks if needed\n\n    avg_acc = (hazard_category_acc + product_category_acc) / 4\n\n\n\n    # Return a dictionary with both accuracy and average F1 score\n\n    return {\n\n        'hazard_category_acc': hazard_category_acc,\n\n        'product_category_acc': product_category_acc,\n\n        'avg_accuracy': avg_acc,\n\n        'avg_f1': avg_f1\n\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T10:00:02.584084Z","iopub.execute_input":"2024-11-30T10:00:02.584366Z","iopub.status.idle":"2024-11-30T10:00:02.598120Z","shell.execute_reply.started":"2024-11-30T10:00:02.584340Z","shell.execute_reply":"2024-11-30T10:00:02.597187Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define the data collator\ndef data_collator(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'hazard_category_labels': torch.tensor([x['hazard_category_labels'] for x in batch]),\n        'product_category_labels': torch.tensor([x['product_category_labels'] for x in batch]),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T10:00:02.599257Z","iopub.execute_input":"2024-11-30T10:00:02.599536Z","iopub.status.idle":"2024-11-30T10:00:02.612069Z","shell.execute_reply.started":"2024-11-30T10:00:02.599510Z","shell.execute_reply":"2024-11-30T10:00:02.611299Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Function to train and save a model\ndef train_and_save_model(model_name, output_dir):\n    \"\"\"\n    Trains and saves a model (only the final model after the last epoch).\n    \n    Args:\n    - model_name: the pre-trained model name or path.\n    - output_dir: directory to save the model\n    \"\"\"\n    # Initialize the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Tokenize the text data\n    train_texts = train_df['text'].tolist()\n    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n\n    val_texts = val_df['text'].tolist()\n    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n    # Prepare the datasets\n    train_dataset = FoodHazardDataset(\n        train_encodings,\n        train_df['hazard-category'].tolist(),\n        train_df['product-category'].tolist()\n        # train_df['hazard'].tolist(),\n        # train_df['product'].tolist()\n    )\n\n    val_dataset = FoodHazardDataset(\n        val_encodings,\n        val_df['hazard-category'].tolist(),\n        val_df['product-category'].tolist()\n        # val_df['hazard'].tolist(),\n        # val_df['product'].tolist()\n    )\n\n    # Define the number of labels\n    num_labels_dict = {\n        'hazard_category': num_hazard_category_labels,\n        'product_category': num_product_category_labels\n        # 'hazard': num_hazard_labels,\n        # 'product': num_product_labels\n    }\n\n    # Initialize the model\n    model = TransformerForFoodHazardClassification(model_name, num_labels_dict)\n\n    # Move the model to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    model.to(device)\n\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=8,  # Train for 8 epochs\n        per_device_train_batch_size=16,  # Adjust based on your GPU memory\n        per_device_eval_batch_size=16,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",  # Do not save after each epoch\n        logging_dir='./logs',\n        logging_steps=10,\n        warmup_steps=500,\n        weight_decay=0.01,\n        report_to=[]  # Disable W&B logging\n    )\n\n    # Initialize the Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n        optimizers=(AdamW(model.parameters(), lr=1e-5), None),\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Save the model only after the last epoch (epoch 8)\n    if model_name == 'allenai/scibert_scivocab_uncased':\n        # Save the model only after the last epoch (epoch 8)\n        state_dict = {k: v.contiguous() if isinstance(v, torch.Tensor) else v for k, v in model.state_dict().items()}\n        torch.save(state_dict, os.path.join(output_dir, \"scibert_weights\"))\n    else:\n        trainer.save_model(output_dir)\n\n    # Evaluate the model\n    eval_results = trainer.evaluate()\n    print(f\"Evaluation results for {model_name}:\")\n    print(eval_results)\n\n    # Clear GPU memory\n    del model\n    torch.cuda.empty_cache()\n\n    return eval_results  # Return evaluation results instead of the trainer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T12:19:19.245418Z","iopub.execute_input":"2024-11-30T12:19:19.245843Z","iopub.status.idle":"2024-11-30T12:19:19.256040Z","shell.execute_reply.started":"2024-11-30T12:19:19.245807Z","shell.execute_reply":"2024-11-30T12:19:19.255067Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# # Plotting function\n# def plot_training_metrics(metrics, output_dir):\n#     epochs = []\n#     train_loss = []\n#     eval_loss = []\n#     eval_accuracy = []\n#     eval_f1 = []\n\n#     for log in metrics:\n#         if 'epoch' in log:\n#             epochs.append(log['epoch'])\n#         if 'loss' in log:\n#             train_loss.append(log['loss'])\n#         if 'eval_loss' in log:\n#             eval_loss.append(log['eval_loss'])\n#         if 'eval_accuracy' in log:\n#             eval_accuracy.append(log['eval_accuracy'])\n#         if 'eval_f1' in log:\n#             eval_f1.append(log['eval_f1'])\n\n#     # Plot Loss\n#     plt.figure(figsize=(10, 6))\n#     plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n#     plt.plot(epochs, eval_loss, label='Eval Loss', marker='x')\n#     plt.xlabel('Epoch')\n#     plt.ylabel('Loss')\n#     plt.title('Loss vs Epochs')\n#     plt.legend()\n#     plt.grid(True)\n#     plt.savefig(f\"{output_dir}/loss_vs_epochs.png\")\n#     plt.show()\n\n#     # Plot Accuracy\n#     plt.figure(figsize=(10, 6))\n#     plt.plot(epochs, eval_accuracy, label='Eval Accuracy', marker='o')\n#     plt.xlabel('Epoch')\n#     plt.ylabel('Accuracy')\n#     plt.title('Accuracy vs Epochs')\n#     plt.legend()\n#     plt.grid(True)\n#     plt.savefig(f\"{output_dir}/accuracy_vs_epochs.png\")\n#     plt.show()\n\n#     # Plot F1 Score\n#     plt.figure(figsize=(10, 6))\n#     plt.plot(epochs, eval_f1, label='Eval F1 Score', marker='x')\n#     plt.xlabel('Epoch')\n#     plt.ylabel('F1 Score')\n#     plt.title('F1 Score vs Epochs')\n#     plt.legend()\n#     plt.grid(True)\n#     plt.savefig(f\"{output_dir}/f1_score_vs_epochs.png\")\n#     plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T10:00:02.629468Z","iopub.execute_input":"2024-11-30T10:00:02.629718Z","iopub.status.idle":"2024-11-30T10:00:02.646931Z","shell.execute_reply.started":"2024-11-30T10:00:02.629694Z","shell.execute_reply":"2024-11-30T10:00:02.646153Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T12:16:33.387165Z","iopub.execute_input":"2024-11-30T12:16:33.387517Z","iopub.status.idle":"2024-11-30T12:16:33.451974Z","shell.execute_reply.started":"2024-11-30T12:16:33.387481Z","shell.execute_reply":"2024-11-30T12:16:33.450989Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Train DeBERTa Large\ntrainer_deberta_base = train_and_save_model('microsoft/deberta-base', 'deberta-base-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T10:00:02.663962Z","iopub.execute_input":"2024-11-30T10:00:02.664224Z","iopub.status.idle":"2024-11-30T11:05:28.356770Z","shell.execute_reply.started":"2024-11-30T10:00:02.664199Z","shell.execute_reply":"2024-11-30T11:05:28.355822Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ed7207a2444e649ced2fcdc62ac6e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2481a804169e4188bdb421492168a950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e725b5050c2428fa7b7d66951ab8464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a2a8d74ec24721b205f0396265ed9a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04cdf27e0302487585e05864d80c66e6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2392' max='2392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2392/2392 1:04:30, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Category Acc</th>\n      <th>Product Category Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.580900</td>\n      <td>3.254996</td>\n      <td>0.659129</td>\n      <td>0.314070</td>\n      <td>0.243300</td>\n      <td>0.188807</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.258600</td>\n      <td>2.152755</td>\n      <td>0.855946</td>\n      <td>0.524288</td>\n      <td>0.345059</td>\n      <td>0.323262</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.579300</td>\n      <td>1.572194</td>\n      <td>0.894472</td>\n      <td>0.645729</td>\n      <td>0.385050</td>\n      <td>0.373890</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.174200</td>\n      <td>1.419254</td>\n      <td>0.897822</td>\n      <td>0.680905</td>\n      <td>0.394682</td>\n      <td>0.389779</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.972000</td>\n      <td>1.329947</td>\n      <td>0.896147</td>\n      <td>0.720268</td>\n      <td>0.404104</td>\n      <td>0.398983</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.730000</td>\n      <td>1.301678</td>\n      <td>0.899497</td>\n      <td>0.724456</td>\n      <td>0.405988</td>\n      <td>0.401012</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.666300</td>\n      <td>1.275241</td>\n      <td>0.906198</td>\n      <td>0.724456</td>\n      <td>0.407663</td>\n      <td>0.404075</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.595500</td>\n      <td>1.271792</td>\n      <td>0.902848</td>\n      <td>0.731156</td>\n      <td>0.408501</td>\n      <td>0.404945</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:39]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for microsoft/deberta-base:\n{'eval_loss': 1.2717922925949097, 'eval_hazard_category_acc': 0.9028475711892797, 'eval_product_category_acc': 0.7311557788944724, 'eval_avg_accuracy': 0.408500837520938, 'eval_avg_f1': 0.40494515026030603, 'eval_runtime': 39.8434, 'eval_samples_per_second': 29.967, 'eval_steps_per_second': 1.882, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# # After training DeBERTa model\n# metrics_deberta = trainer_deberta_base.state.log_history\n# plot_training_metrics(metrics_deberta, 'deberta-base-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:55:06.010350Z","iopub.status.idle":"2024-11-30T09:55:06.010632Z","shell.execute_reply.started":"2024-11-30T09:55:06.010491Z","shell.execute_reply":"2024-11-30T09:55:06.010505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T11:06:42.712283Z","iopub.execute_input":"2024-11-30T11:06:42.712636Z","iopub.status.idle":"2024-11-30T11:06:42.782886Z","shell.execute_reply.started":"2024-11-30T11:06:42.712605Z","shell.execute_reply":"2024-11-30T11:06:42.781934Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"trainer_distilbert_base = train_and_save_model('distilbert-base-uncased', 'distilbert-base-uncased-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T11:07:07.279811Z","iopub.execute_input":"2024-11-30T11:07:07.280168Z","iopub.status.idle":"2024-11-30T11:27:01.087644Z","shell.execute_reply.started":"2024-11-30T11:07:07.280138Z","shell.execute_reply":"2024-11-30T11:27:01.086662Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d2438e773804658ac04acf6120c88b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b98bf01cf5f74c579ff995b1a9fedcf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f00c291a49264cc3b661fcbb19daeceb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5d88f44eb1c44eeaf49373c7310bfad"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef7311a12a7f4b2eb100f9f1b544965d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 19:32, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Category Acc</th>\n      <th>Product Category Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.223900</td>\n      <td>4.082211</td>\n      <td>0.531826</td>\n      <td>0.286432</td>\n      <td>0.204564</td>\n      <td>0.138292</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.271600</td>\n      <td>3.172018</td>\n      <td>0.703518</td>\n      <td>0.328308</td>\n      <td>0.257956</td>\n      <td>0.210897</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.593700</td>\n      <td>2.563595</td>\n      <td>0.860134</td>\n      <td>0.386097</td>\n      <td>0.311558</td>\n      <td>0.275712</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.185700</td>\n      <td>2.135347</td>\n      <td>0.869347</td>\n      <td>0.526801</td>\n      <td>0.349037</td>\n      <td>0.329341</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.008100</td>\n      <td>1.887733</td>\n      <td>0.875209</td>\n      <td>0.576214</td>\n      <td>0.362856</td>\n      <td>0.345559</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.522600</td>\n      <td>1.779248</td>\n      <td>0.877722</td>\n      <td>0.603015</td>\n      <td>0.370184</td>\n      <td>0.355818</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.270000</td>\n      <td>1.718544</td>\n      <td>0.884422</td>\n      <td>0.621441</td>\n      <td>0.376466</td>\n      <td>0.364454</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.509100</td>\n      <td>1.698771</td>\n      <td>0.888610</td>\n      <td>0.627303</td>\n      <td>0.378978</td>\n      <td>0.367598</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:12]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for distilbert-base-uncased:\n{'eval_loss': 1.6987708806991577, 'eval_hazard_category_acc': 0.8886097152428811, 'eval_product_category_acc': 0.6273031825795645, 'eval_avg_accuracy': 0.3789782244556114, 'eval_avg_f1': 0.3675979446969274, 'eval_runtime': 12.3662, 'eval_samples_per_second': 96.554, 'eval_steps_per_second': 3.073, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# # metrics_distilbert = trainer_distilbert_base.state.log_history\n# plot_training_metrics(trainer_distilbert_base, 'distilbert-base-uncased-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:12:28.449707Z","iopub.execute_input":"2024-11-27T17:12:28.450034Z","iopub.status.idle":"2024-11-27T17:12:28.453880Z","shell.execute_reply.started":"2024-11-27T17:12:28.450006Z","shell.execute_reply":"2024-11-27T17:12:28.453022Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T12:19:26.508012Z","iopub.execute_input":"2024-11-30T12:19:26.508716Z","iopub.status.idle":"2024-11-30T12:19:26.803703Z","shell.execute_reply.started":"2024-11-30T12:19:26.508681Z","shell.execute_reply":"2024-11-30T12:19:26.802678Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"trainer_scibert = train_and_save_model('allenai/scibert_scivocab_uncased', 'scibert_scivocab_uncased-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T12:19:26.805685Z","iopub.execute_input":"2024-11-30T12:19:26.806354Z","iopub.status.idle":"2024-11-30T12:57:44.140142Z","shell.execute_reply.started":"2024-11-30T12:19:26.806299Z","shell.execute_reply":"2024-11-30T12:57:44.138981Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 37:47, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Category Acc</th>\n      <th>Product Category Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.239100</td>\n      <td>4.080337</td>\n      <td>0.604690</td>\n      <td>0.287270</td>\n      <td>0.222990</td>\n      <td>0.153340</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.018500</td>\n      <td>2.989182</td>\n      <td>0.814070</td>\n      <td>0.335008</td>\n      <td>0.287270</td>\n      <td>0.244808</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.495600</td>\n      <td>2.509274</td>\n      <td>0.878559</td>\n      <td>0.403685</td>\n      <td>0.320561</td>\n      <td>0.286823</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.240700</td>\n      <td>2.197078</td>\n      <td>0.883585</td>\n      <td>0.473199</td>\n      <td>0.339196</td>\n      <td>0.311013</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.961400</td>\n      <td>1.970347</td>\n      <td>0.887772</td>\n      <td>0.557789</td>\n      <td>0.361390</td>\n      <td>0.341576</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.604100</td>\n      <td>1.881533</td>\n      <td>0.889447</td>\n      <td>0.571189</td>\n      <td>0.365159</td>\n      <td>0.346247</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.327600</td>\n      <td>1.794378</td>\n      <td>0.891960</td>\n      <td>0.612228</td>\n      <td>0.376047</td>\n      <td>0.361425</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.488000</td>\n      <td>1.780465</td>\n      <td>0.891122</td>\n      <td>0.620603</td>\n      <td>0.377931</td>\n      <td>0.363556</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:20]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for allenai/scibert_scivocab_uncased:\n{'eval_loss': 1.7804646492004395, 'eval_hazard_category_acc': 0.8911222780569514, 'eval_product_category_acc': 0.6206030150753769, 'eval_avg_accuracy': 0.3779313232830821, 'eval_avg_f1': 0.36355608186465943, 'eval_runtime': 20.9991, 'eval_samples_per_second': 56.86, 'eval_steps_per_second': 1.81, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# # After training SciBERT model\n# metrics_scibert = trainer_scibert.state.log_history\n# plot_training_metrics(metrics_scibert, 'scibert_scivocab_uncased-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T12:57:44.142362Z","iopub.execute_input":"2024-11-30T12:57:44.142797Z","iopub.status.idle":"2024-11-30T12:57:44.147203Z","shell.execute_reply.started":"2024-11-30T12:57:44.142728Z","shell.execute_reply":"2024-11-30T12:57:44.146318Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T12:57:44.148586Z","iopub.execute_input":"2024-11-30T12:57:44.149092Z","iopub.status.idle":"2024-11-30T12:57:44.207340Z","shell.execute_reply.started":"2024-11-30T12:57:44.149052Z","shell.execute_reply":"2024-11-30T12:57:44.206485Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Load the test data (validation data is actually the test data)\ntest_df = pd.read_csv('/kaggle/working/final_cleaned_validation.csv')\n\n# Drop unnecessary columns\ntest_df = test_df[['text']]\ntest_texts = test_df['text'].tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T12:57:44.209291Z","iopub.execute_input":"2024-11-30T12:57:44.209628Z","iopub.status.idle":"2024-11-30T12:57:44.238056Z","shell.execute_reply.started":"2024-11-30T12:57:44.209597Z","shell.execute_reply":"2024-11-30T12:57:44.237233Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nfrom safetensors.torch import load_file\nimport numpy as np\nimport torch\n\ndef get_model_logits(model_name, model_dir, test_texts, batch_size=8):\n    # Initialize the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Tokenize the test data\n    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n\n    # Convert tokenized inputs to a TensorDataset\n    test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'])\n\n    # Use DataLoader to load the data in batches\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # Define the number of labels\n    num_labels_dict = {\n        'hazard_category': num_hazard_category_labels,\n        'product_category': num_product_category_labels\n    }\n\n    # Initialize the model\n    model = TransformerForFoodHazardClassification(model_name, num_labels_dict)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Load the model state dict from model.safetensors\n    if model_name == \"allenai/scibert_scivocab_uncased\":\n        state_dict = torch.load(f\"{model_dir}/scibert_weights\", map_location=device)\n        model.load_state_dict(state_dict)\n    else:\n        state_dict = load_file(f\"{model_dir}/model.safetensors\")\n        model.load_state_dict(state_dict)\n\n    # Move model to the GPUs (using DataParallel for multiple GPUs)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  # Default to cuda:0\n    model = torch.nn.DataParallel(model, device_ids=[0, 1])  # Use both GPU 0 and GPU 1\n    model.to(device)\n\n    # Initialize dictionaries to accumulate logits\n    all_hazard_category_logits = []\n    all_product_category_logits = []\n\n    with torch.no_grad():\n        model.eval()\n        for batch in test_dataloader:\n            input_ids, attention_mask = [b.to(device) for b in batch]\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n            # Unpack logits and move them to CPU\n            hazard_category_logits, product_category_logits = outputs\n            all_hazard_category_logits.append(hazard_category_logits.cpu().numpy())\n            all_product_category_logits.append(product_category_logits.cpu().numpy())\n            # all_hazard_logits.append(hazard_logits.cpu().numpy())\n            # all_product_logits.append(product_logits.cpu().numpy())\n\n    # Concatenate all logits from batches along the batch dimension (axis=0)\n    hazard_category_logits_concat = np.concatenate(all_hazard_category_logits, axis=0)\n    product_category_logits_concat = np.concatenate(all_product_category_logits, axis=0)\n    # hazard_logits_concat = np.concatenate(all_hazard_logits, axis=0)\n    # product_logits_concat = np.concatenate(all_product_logits, axis=0)\n\n    # Free GPU memory by deleting model and test_encodings\n    del model\n    del test_encodings\n\n    # Clear CUDA cache\n    torch.cuda.empty_cache()\n\n    # Return logits as a structured dictionary\n    return {\n        'hazard_category': hazard_category_logits_concat,\n        'product_category': product_category_logits_concat\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:09:23.954866Z","iopub.execute_input":"2024-11-30T20:09:23.955196Z","iopub.status.idle":"2024-11-30T20:09:23.965588Z","shell.execute_reply.started":"2024-11-30T20:09:23.955165Z","shell.execute_reply":"2024-11-30T20:09:23.964610Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# After evaluating the model\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:08:07.034564Z","iopub.execute_input":"2024-11-30T20:08:07.035282Z","iopub.status.idle":"2024-11-30T20:08:07.039552Z","shell.execute_reply.started":"2024-11-30T20:08:07.035246Z","shell.execute_reply":"2024-11-30T20:08:07.038561Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Get logits from DeBERTa Large\ndeberta_base_logits = get_model_logits('microsoft/deberta-base', 'deberta-base-model', test_texts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:01:05.668778Z","iopub.execute_input":"2024-11-30T20:01:05.669404Z","iopub.status.idle":"2024-11-30T20:01:31.285384Z","shell.execute_reply.started":"2024-11-30T20:01:05.669373Z","shell.execute_reply":"2024-11-30T20:01:31.284600Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed54a8ccc62d4b8da3e49fe7abb37c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97aa66dbd78844f29d205dae24f53a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e4bc337890b4e3b9283e337602a176c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09132618c24f40aa832364155a3285fd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e09b7620a454d07afb79c573821f255"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# After evaluating the model\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:01:31.368554Z","iopub.execute_input":"2024-11-30T20:01:31.368800Z","iopub.status.idle":"2024-11-30T20:01:31.373082Z","shell.execute_reply.started":"2024-11-30T20:01:31.368774Z","shell.execute_reply":"2024-11-30T20:01:31.372284Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"distilbert_base_logits = get_model_logits('distilbert-base-uncased', 'distilbert-base-uncased-model', test_texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:01:31.392584Z","iopub.execute_input":"2024-11-30T20:01:31.392819Z","iopub.status.idle":"2024-11-30T20:01:40.218102Z","shell.execute_reply.started":"2024-11-30T20:01:31.392795Z","shell.execute_reply":"2024-11-30T20:01:40.217325Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90824433d7b541488726e5980d33fa4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa2e4db8b7394ebcb57aae1dabcba279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23404b46ec6942ee83f9546cd3479171"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"266caa0ddf534f539cce79dcd2aa4ac0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29ea1071bcae41999477eb120ea54aa5"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# After evaluating the model\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:08:46.513326Z","iopub.execute_input":"2024-11-30T20:08:46.514171Z","iopub.status.idle":"2024-11-30T20:08:46.518306Z","shell.execute_reply.started":"2024-11-30T20:08:46.514136Z","shell.execute_reply":"2024-11-30T20:08:46.517491Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"scibert_logits = get_model_logits('allenai/scibert_scivocab_uncased', 'scibert_scivocab_uncased-model', test_texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:09:27.003617Z","iopub.execute_input":"2024-11-30T20:09:27.004412Z","iopub.status.idle":"2024-11-30T20:09:39.581614Z","shell.execute_reply.started":"2024-11-30T20:09:27.004381Z","shell.execute_reply":"2024-11-30T20:09:39.580861Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3498721307.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(f\"{model_dir}/scibert_weights\", map_location=device)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Average the logits\nhazard_category_logits_avg = (distilbert_base_logits['hazard_category'] + deberta_base_logits['hazard_category'] + scibert_logits['hazard_category']) / 3\nproduct_category_logits_avg = (distilbert_base_logits['product_category'] + deberta_base_logits['product_category'] + scibert_logits['product_category']) / 3\n# hazard_logits_avg = (distilbert_base_logits['hazard'] + deberta_large_logits['hazard']) / 2\n# product_logits_avg = (distilbert_base_logits['product'] + deberta_large_logits['product']) / 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:11:13.116289Z","iopub.execute_input":"2024-11-30T20:11:13.116663Z","iopub.status.idle":"2024-11-30T20:11:13.121857Z","shell.execute_reply.started":"2024-11-30T20:11:13.116631Z","shell.execute_reply":"2024-11-30T20:11:13.120959Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Get predicted labels\nhazard_category_preds = np.argmax(hazard_category_logits_avg, axis=1)\nproduct_category_preds = np.argmax(product_category_logits_avg, axis=1)\n# hazard_preds = np.argmax(hazard_logits_avg, axis=1)\n# product_preds = np.argmax(product_logits_avg, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:11:14.373309Z","iopub.execute_input":"2024-11-30T20:11:14.373656Z","iopub.status.idle":"2024-11-30T20:11:14.384605Z","shell.execute_reply.started":"2024-11-30T20:11:14.373627Z","shell.execute_reply":"2024-11-30T20:11:14.383828Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Decode the predicted labels using the label encoders\nhazard_category_labels = hazard_category_encoder.inverse_transform(hazard_category_preds)\nproduct_category_labels = product_category_encoder.inverse_transform(product_category_preds)\n# hazard_labels = hazard_encoder.inverse_transform(hazard_preds)\n# product_labels = product_encoder.inverse_transform(product_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:11:15.609361Z","iopub.execute_input":"2024-11-30T20:11:15.609748Z","iopub.status.idle":"2024-11-30T20:11:15.624391Z","shell.execute_reply.started":"2024-11-30T20:11:15.609707Z","shell.execute_reply":"2024-11-30T20:11:15.623503Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Create a DataFrame for the predictions\noutput_df = pd.DataFrame({\n    'hazard-category': hazard_category_labels,\n    'product-category': product_category_labels\n})\n\n# Save the output DataFrame to a CSV file\noutput_df.to_csv('test_predictions_ensemble.csv', index=False)\n\n# For subtask 1 (hazard-category and product-category)\nsubtask1_df = output_df[['hazard-category', 'product-category']]\nsubtask1_df.to_csv('subtask1_predictions_ensemble.csv', index=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:11:16.045522Z","iopub.execute_input":"2024-11-30T20:11:16.045904Z","iopub.status.idle":"2024-11-30T20:11:16.095727Z","shell.execute_reply.started":"2024-11-30T20:11:16.045872Z","shell.execute_reply":"2024-11-30T20:11:16.095023Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Analyze the predictions\nprint(\"Hazard Category Predictions:\")\nprint(subtask1_df['hazard-category'].value_counts())\n\nprint(\"\\nProduct Category Predictions:\")\nprint(subtask1_df['product-category'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:11:16.821635Z","iopub.execute_input":"2024-11-30T20:11:16.822035Z","iopub.status.idle":"2024-11-30T20:11:16.839402Z","shell.execute_reply.started":"2024-11-30T20:11:16.822004Z","shell.execute_reply":"2024-11-30T20:11:16.838391Z"}},"outputs":[{"name":"stdout","text":"Hazard Category Predictions:\nhazard-category\nallergens                         222\nbiological                        197\nforeign bodies                     64\nfraud                              34\nchemical                           31\npackaging defect                    7\nother hazard                        5\norganoleptic aspects                4\nfood additives and flavourings      1\nName: count, dtype: int64\n\nProduct Category Predictions:\nproduct-category\nmeat, egg and dairy products                         171\ncereals and bakery products                           90\nfruits and vegetables                                 71\nprepared dishes and snacks                            42\nnuts, nut products and seeds                          37\nseafood                                               26\nsoups, broths, sauces and condiments                  26\nices and desserts                                     23\nnon-alcoholic beverages                               23\ncocoa and cocoa preparations, coffee and tea          19\ndietetic foods, food supplements, fortified foods     10\nherbs and spices                                       9\nalcoholic beverages                                    8\nconfectionery                                          7\nfats and oils                                          2\nother food product / mixed                             1\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}