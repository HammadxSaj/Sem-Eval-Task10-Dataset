{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n!pip install transformers datasets accelerate\n!pip install transformers[torch]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T15:51:17.837962Z","iopub.execute_input":"2024-11-17T15:51:17.838967Z","iopub.status.idle":"2024-11-17T15:51:43.582057Z","shell.execute_reply.started":"2024-11-17T15:51:17.838910Z","shell.execute_reply":"2024-11-17T15:51:43.580887Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\nRequirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.34.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.4.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import libraries\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, Trainer, TrainingArguments, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:22.264810Z","iopub.execute_input":"2024-11-21T05:40:22.265695Z","iopub.status.idle":"2024-11-21T05:40:22.270736Z","shell.execute_reply.started":"2024-11-21T05:40:22.265661Z","shell.execute_reply":"2024-11-21T05:40:22.269886Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Download datasets\n!wget -P /kaggle/working -nc \"https://raw.githubusercontent.com/HammadxSaj/Sem-Eval-Task10-Dataset/refs/heads/main/final_cleaned_train.csv\"\n!wget -P /kaggle/working -nc \"https://raw.githubusercontent.com/HammadxSaj/Sem-Eval-Task10-Dataset/refs/heads/main/final_cleaned_validation.csv\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:23.256425Z","iopub.execute_input":"2024-11-21T05:40:23.257230Z","iopub.status.idle":"2024-11-21T05:40:25.309281Z","shell.execute_reply.started":"2024-11-21T05:40:23.257198Z","shell.execute_reply":"2024-11-21T05:40:25.307928Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"File '/kaggle/working/final_cleaned_train.csv' already there; not retrieving.\n\nFile '/kaggle/working/final_cleaned_validation.csv' already there; not retrieving.\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Load the training data\ndf = pd.read_csv('/kaggle/working/final_cleaned_train.csv')\n\n# Inspect the dataframe\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:25.311669Z","iopub.execute_input":"2024-11-21T05:40:25.312063Z","iopub.status.idle":"2024-11-21T05:40:25.511307Z","shell.execute_reply.started":"2024-11-21T05:40:25.312024Z","shell.execute_reply":"2024-11-21T05:40:25.510256Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   year  month  day country                             title  \\\n0  1994      1    7      us  Recall Notification: FSIS-024-94   \n1  1994      3   10      us  Recall Notification: FSIS-033-94   \n2  1994      3   28      us  Recall Notification: FSIS-014-94   \n3  1994      4    3      us  Recall Notification: FSIS-009-94   \n4  1994      7    1      us  Recall Notification: FSIS-001-94   \n\n                                                text hazard-category  \\\n0  Date Opened: Date Closed: Name: GERHARD'S NAPA...      biological   \n1  Date Opened: Date Closed: Name: WIMMER'S MEAT ...      biological   \n2  Date Opened: Date Closed: Name: WILLOW FOODS I...      biological   \n3  Date Opened: Date Closed: M Name: OSCAR MAYER ...  foreign bodies   \n4  Date Opened: Date Closed: Name: TYSON FOODS Im...  foreign bodies   \n\n               product-category                  hazard  \\\n0  meat, egg and dairy products  listeria monocytogenes   \n1  meat, egg and dairy products            listeria spp   \n2  meat, egg and dairy products  listeria monocytogenes   \n3  meat, egg and dairy products        plastic fragment   \n4  meat, egg and dairy products        plastic fragment   \n\n                       product  \n0               smoked sausage  \n1                      sausage  \n2                   ham slices  \n3  thermal processed pork meat  \n4               chicken breast  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>country</th>\n      <th>title</th>\n      <th>text</th>\n      <th>hazard-category</th>\n      <th>product-category</th>\n      <th>hazard</th>\n      <th>product</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1994</td>\n      <td>1</td>\n      <td>7</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-024-94</td>\n      <td>Date Opened: Date Closed: Name: GERHARD'S NAPA...</td>\n      <td>biological</td>\n      <td>meat, egg and dairy products</td>\n      <td>listeria monocytogenes</td>\n      <td>smoked sausage</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1994</td>\n      <td>3</td>\n      <td>10</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-033-94</td>\n      <td>Date Opened: Date Closed: Name: WIMMER'S MEAT ...</td>\n      <td>biological</td>\n      <td>meat, egg and dairy products</td>\n      <td>listeria spp</td>\n      <td>sausage</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1994</td>\n      <td>3</td>\n      <td>28</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-014-94</td>\n      <td>Date Opened: Date Closed: Name: WILLOW FOODS I...</td>\n      <td>biological</td>\n      <td>meat, egg and dairy products</td>\n      <td>listeria monocytogenes</td>\n      <td>ham slices</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1994</td>\n      <td>4</td>\n      <td>3</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-009-94</td>\n      <td>Date Opened: Date Closed: M Name: OSCAR MAYER ...</td>\n      <td>foreign bodies</td>\n      <td>meat, egg and dairy products</td>\n      <td>plastic fragment</td>\n      <td>thermal processed pork meat</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1994</td>\n      <td>7</td>\n      <td>1</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-001-94</td>\n      <td>Date Opened: Date Closed: Name: TYSON FOODS Im...</td>\n      <td>foreign bodies</td>\n      <td>meat, egg and dairy products</td>\n      <td>plastic fragment</td>\n      <td>chicken breast</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Data preprocessing\n\n# Drop unnecessary columns for training\ndf = df[['text', 'hazard-category', 'product-category', 'hazard', 'product']]\n\n# Drop rows with missing values\ndf.dropna(inplace=True)\n\n# Initialize label encoders\nhazard_category_encoder = LabelEncoder()\nproduct_category_encoder = LabelEncoder()\nhazard_encoder = LabelEncoder()\nproduct_encoder = LabelEncoder()\n\n# Fit the encoders\nhazard_category_encoder.fit(df['hazard-category'])\nproduct_category_encoder.fit(df['product-category'])\nhazard_encoder.fit(df['hazard'])\nproduct_encoder.fit(df['product'])\n\n# Transform the labels\ndf['hazard-category'] = hazard_category_encoder.transform(df['hazard-category'])\ndf['product-category'] = product_category_encoder.transform(df['product-category'])\ndf['hazard'] = hazard_encoder.transform(df['hazard'])\ndf['product'] = product_encoder.transform(df['product'])\n\n# Split the data into train and validation sets\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(f\"Number of training samples: {len(train_df)}\")\nprint(f\"Number of validation samples: {len(val_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:25.512765Z","iopub.execute_input":"2024-11-21T05:40:25.513235Z","iopub.status.idle":"2024-11-21T05:40:25.551047Z","shell.execute_reply.started":"2024-11-21T05:40:25.513188Z","shell.execute_reply":"2024-11-21T05:40:25.550033Z"}},"outputs":[{"name":"stdout","text":"Number of training samples: 4772\nNumber of validation samples: 1194\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Define the FoodHazardDataset class\nclass FoodHazardDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, hazard_categories, product_categories, hazards, products):\n        self.encodings = encodings\n        self.hazard_categories = hazard_categories\n        self.product_categories = product_categories\n        self.hazards = hazards\n        self.products = products\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['hazard_category_labels'] = torch.tensor(self.hazard_categories[idx])\n        item['product_category_labels'] = torch.tensor(self.product_categories[idx])\n        item['hazard_labels'] = torch.tensor(self.hazards[idx])\n        item['product_labels'] = torch.tensor(self.products[idx])\n        return item\n\n    def __len__(self):\n        return len(self.hazard_categories)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:25.934287Z","iopub.execute_input":"2024-11-21T05:40:25.935250Z","iopub.status.idle":"2024-11-21T05:40:25.944232Z","shell.execute_reply.started":"2024-11-21T05:40:25.935202Z","shell.execute_reply":"2024-11-21T05:40:25.943340Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define the number of unique labels for each category\nnum_hazard_category_labels = len(hazard_category_encoder.classes_)\nnum_product_category_labels = len(product_category_encoder.classes_)\nnum_hazard_labels = len(hazard_encoder.classes_)\nnum_product_labels = len(product_encoder.classes_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:26.506914Z","iopub.execute_input":"2024-11-21T05:40:26.507282Z","iopub.status.idle":"2024-11-21T05:40:26.511941Z","shell.execute_reply.started":"2024-11-21T05:40:26.507252Z","shell.execute_reply":"2024-11-21T05:40:26.510963Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from transformers import AutoModel\nimport torch.nn as nn\n\nclass TransformerForFoodHazardClassification(nn.Module):\n    def __init__(self, model_name, num_labels_dict):\n        super().__init__()\n        self.transformer = AutoModel.from_pretrained(model_name)\n        # Uncomment the line below if you want to use dropout\n        # self.dropout = nn.Dropout(self.transformer.config.hidden_dropout_prob)\n\n        hidden_size = self.transformer.config.hidden_size\n\n        # Classifiers for the four labels\n        self.hazard_category_classifier = nn.Linear(hidden_size, num_labels_dict['hazard_category'])\n        self.product_category_classifier = nn.Linear(hidden_size, num_labels_dict['product_category'])\n        self.hazard_classifier = nn.Linear(hidden_size, num_labels_dict['hazard'])\n        self.product_classifier = nn.Linear(hidden_size, num_labels_dict['product'])\n\n        # Loss function\n        self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                hazard_category_labels=None, product_category_labels=None,\n                hazard_labels=None, product_labels=None):\n        # Check if the model supports token_type_ids\n        if \"token_type_ids\" in self.transformer.forward.__code__.co_varnames:\n            outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        else:\n            # For DistilBERT and similar models that do not accept token_type_ids\n            outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n\n        # Select pooled output for models like BERT and DeBERTa, or use CLS token for others\n        if hasattr(outputs, 'pooler_output'):\n            pooled_output = outputs.pooler_output\n        else:\n            pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n\n        # Apply dropout if using\n        # pooled_output = self.dropout(pooled_output)\n\n        # Predict the four labels\n        hazard_category_logits = self.hazard_category_classifier(pooled_output)\n        product_category_logits = self.product_category_classifier(pooled_output)\n        hazard_logits = self.hazard_classifier(pooled_output)\n        product_logits = self.product_classifier(pooled_output)\n\n        loss = None\n        if hazard_category_labels is not None and product_category_labels is not None \\\n           and hazard_labels is not None and product_labels is not None:\n            # Compute loss for each task\n            hazard_category_loss = self.loss_fct(hazard_category_logits, hazard_category_labels)\n            product_category_loss = self.loss_fct(product_category_logits, product_category_labels)\n            hazard_loss = self.loss_fct(hazard_logits, hazard_labels)\n            product_loss = self.loss_fct(product_logits, product_labels)\n\n            # Aggregate losses\n            loss = hazard_category_loss + product_category_loss + hazard_loss + product_loss\n\n        # Return the loss and logits\n        output = (hazard_category_logits, product_category_logits, hazard_logits, product_logits)\n        return ((loss,) + output) if loss is not None else output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:27.040435Z","iopub.execute_input":"2024-11-21T05:40:27.041306Z","iopub.status.idle":"2024-11-21T05:40:27.053096Z","shell.execute_reply.started":"2024-11-21T05:40:27.041272Z","shell.execute_reply":"2024-11-21T05:40:27.052098Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Define the compute_metrics function to calculate both accuracy and average F1 score across all labels\n\ndef compute_metrics(pred):\n\n    labels = pred.label_ids\n\n    preds = pred.predictions\n\n\n\n    # Unpack labels and predictions for each task\n\n    hazard_category_labels = labels[0]\n\n    product_category_labels = labels[1]\n\n    hazard_labels = labels[2]\n\n    product_labels = labels[3]\n\n\n\n    hazard_category_preds = preds[0].argmax(-1)\n\n    product_category_preds = preds[1].argmax(-1)\n\n    hazard_preds = preds[2].argmax(-1)\n\n    product_preds = preds[3].argmax(-1)\n\n\n\n    # Compute accuracy for each task (can be used separately if needed)\n\n    hazard_category_acc = accuracy_score(hazard_category_labels, hazard_category_preds)\n\n    product_category_acc = accuracy_score(product_category_labels, product_category_preds)\n\n    hazard_acc = accuracy_score(hazard_labels, hazard_preds)\n\n    product_acc = accuracy_score(product_labels, product_preds)\n\n\n\n    # Compute F1 score for each task\n\n    hazard_category_f1 = f1_score(hazard_category_labels, hazard_category_preds, average='weighted')\n\n    product_category_f1 = f1_score(product_category_labels, product_category_preds, average='weighted')\n\n    hazard_f1 = f1_score(hazard_labels, hazard_preds, average='weighted')\n\n    product_f1 = f1_score(product_labels, product_preds, average='weighted')\n\n\n\n    # Compute average F1 score across all tasks\n\n    avg_f1 = (hazard_category_f1 + product_category_f1 + hazard_f1 + product_f1) / 4\n\n\n\n    # Optionally, you can also compute average accuracy across tasks if needed\n\n    avg_acc = (hazard_category_acc + product_category_acc + hazard_acc + product_acc) / 4\n\n\n\n    # Return a dictionary with both accuracy and average F1 score\n\n    return {\n\n        'hazard_category_acc': hazard_category_acc,\n\n        'product_category_acc': product_category_acc,\n\n        'hazard_acc': hazard_acc,\n\n        'product_acc': product_acc,\n\n        'avg_accuracy': avg_acc,\n\n        'avg_f1': avg_f1\n\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:27.638914Z","iopub.execute_input":"2024-11-21T05:40:27.639717Z","iopub.status.idle":"2024-11-21T05:40:27.646758Z","shell.execute_reply.started":"2024-11-21T05:40:27.639684Z","shell.execute_reply":"2024-11-21T05:40:27.645925Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Define the data collator\ndef data_collator(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'hazard_category_labels': torch.tensor([x['hazard_category_labels'] for x in batch]),\n        'product_category_labels': torch.tensor([x['product_category_labels'] for x in batch]),\n        'hazard_labels': torch.tensor([x['hazard_labels'] for x in batch]),\n        'product_labels': torch.tensor([x['product_labels'] for x in batch]),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:28.145224Z","iopub.execute_input":"2024-11-21T05:40:28.145576Z","iopub.status.idle":"2024-11-21T05:40:28.151032Z","shell.execute_reply.started":"2024-11-21T05:40:28.145539Z","shell.execute_reply":"2024-11-21T05:40:28.150117Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Disable W&B entirely\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:28.658261Z","iopub.execute_input":"2024-11-21T05:40:28.658948Z","iopub.status.idle":"2024-11-21T05:40:28.662914Z","shell.execute_reply.started":"2024-11-21T05:40:28.658914Z","shell.execute_reply":"2024-11-21T05:40:28.661943Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Function to train and save a model\ndef train_and_save_model(model_name, output_dir):\n    \"\"\"\n    Trains and saves a model (only the final model after the last epoch).\n    \n    Args:\n    - model_name: the pre-trained model name or path.\n    - output_dir: directory to save the model\n    \"\"\"\n    # Initialize the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Tokenize the text data\n    train_texts = train_df['text'].tolist()\n    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n\n    val_texts = val_df['text'].tolist()\n    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n    # Prepare the datasets\n    train_dataset = FoodHazardDataset(\n        train_encodings,\n        train_df['hazard-category'].tolist(),\n        train_df['product-category'].tolist(),\n        train_df['hazard'].tolist(),\n        train_df['product'].tolist()\n    )\n\n    val_dataset = FoodHazardDataset(\n        val_encodings,\n        val_df['hazard-category'].tolist(),\n        val_df['product-category'].tolist(),\n        val_df['hazard'].tolist(),\n        val_df['product'].tolist()\n    )\n\n    # Define the number of labels\n    num_labels_dict = {\n        'hazard_category': num_hazard_category_labels,\n        'product_category': num_product_category_labels,\n        'hazard': num_hazard_labels,\n        'product': num_product_labels\n    }\n\n    # Initialize the model\n    model = TransformerForFoodHazardClassification(model_name, num_labels_dict)\n\n    # Move the model to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    model.to(device)\n\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=8,  # Train for 8 epochs\n        per_device_train_batch_size=8,  # Adjust based on your GPU memory\n        per_device_eval_batch_size=8,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",  # Do not save after each epoch\n        logging_dir='./logs',\n        logging_steps=10,\n        warmup_steps=500,\n        weight_decay=0.01,\n        report_to=[]  # Disable W&B logging\n    )\n\n    # Initialize the Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n        optimizers=(AdamW(model.parameters(), lr=1e-5), None),\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Save the model only after the last epoch (epoch 8)\n    if model_name == 'allenai/scibert_scivocab_uncased':\n        # Save the model only after the last epoch (epoch 8)\n        state_dict = {k: v.contiguous() if isinstance(v, torch.Tensor) else v for k, v in model.state_dict().items()}\n        torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n    else:\n        trainer.save_model(output_dir)\n\n    # Evaluate the model\n    eval_results = trainer.evaluate()\n    print(f\"Evaluation results for {model_name}:\")\n    print(eval_results)\n\n    # Clear GPU memory\n    del model\n    torch.cuda.empty_cache()\n\n    return eval_results  # Return evaluation results instead of the trainer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:29.268911Z","iopub.execute_input":"2024-11-21T05:40:29.269608Z","iopub.status.idle":"2024-11-21T05:40:29.278809Z","shell.execute_reply.started":"2024-11-21T05:40:29.269576Z","shell.execute_reply":"2024-11-21T05:40:29.278042Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"trainer_scibert = train_and_save_model('allenai/scibert_scivocab_uncased', 'scibert_scivocab_uncased-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T05:40:30.091399Z","iopub.execute_input":"2024-11-21T05:40:30.091720Z","iopub.status.idle":"2024-11-21T06:20:59.157396Z","shell.execute_reply.started":"2024-11-21T05:40:30.091694Z","shell.execute_reply":"2024-11-21T06:20:59.154582Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8fc0111d4d43eab8ef41f37d2d6d15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f0c0e05fed4148ad94aa00ee92ce5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f696bf4ac31d4e26b0b043aeec3f1847"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2392' max='2392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2392/2392 40:17, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Category Acc</th>\n      <th>Product Category Acc</th>\n      <th>Hazard Acc</th>\n      <th>Product Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>15.070700</td>\n      <td>14.560096</td>\n      <td>0.649079</td>\n      <td>0.304020</td>\n      <td>0.326633</td>\n      <td>0.030988</td>\n      <td>0.327680</td>\n      <td>0.216800</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>11.895100</td>\n      <td>11.918249</td>\n      <td>0.878559</td>\n      <td>0.359296</td>\n      <td>0.580402</td>\n      <td>0.037688</td>\n      <td>0.463987</td>\n      <td>0.388071</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>11.131000</td>\n      <td>10.921135</td>\n      <td>0.890285</td>\n      <td>0.409548</td>\n      <td>0.664154</td>\n      <td>0.043551</td>\n      <td>0.501884</td>\n      <td>0.437529</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>9.926900</td>\n      <td>10.376544</td>\n      <td>0.895310</td>\n      <td>0.456449</td>\n      <td>0.680905</td>\n      <td>0.049414</td>\n      <td>0.520519</td>\n      <td>0.465037</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>9.473400</td>\n      <td>10.028104</td>\n      <td>0.902010</td>\n      <td>0.489112</td>\n      <td>0.697655</td>\n      <td>0.073702</td>\n      <td>0.540620</td>\n      <td>0.485218</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>9.011600</td>\n      <td>9.835027</td>\n      <td>0.905360</td>\n      <td>0.500000</td>\n      <td>0.699330</td>\n      <td>0.086265</td>\n      <td>0.547739</td>\n      <td>0.492300</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>8.749800</td>\n      <td>9.707090</td>\n      <td>0.905360</td>\n      <td>0.509213</td>\n      <td>0.709380</td>\n      <td>0.100503</td>\n      <td>0.556114</td>\n      <td>0.501549</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>9.312700</td>\n      <td>9.668348</td>\n      <td>0.907873</td>\n      <td>0.518425</td>\n      <td>0.711055</td>\n      <td>0.106365</td>\n      <td>0.560930</td>\n      <td>0.506012</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_scibert \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallenai/scibert_scivocab_uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscibert_scivocab_uncased-model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[15], line 83\u001b[0m, in \u001b[0;36mtrain_and_save_model\u001b[0;34m(model_name, output_dir)\u001b[0m\n\u001b[1;32m     80\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Save the model only after the last epoch (epoch 8)\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     86\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3623\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3620\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3625\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3721\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3719\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer.model is not a `PreTrainedModel`, only saving its state dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_safetensors:\n\u001b[0;32m-> 3721\u001b[0m     \u001b[43msafetensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFE_WEIGHTS_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3724\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3725\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/safetensors/torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/safetensors/torch.py:496\u001b[0m, in \u001b[0;36m_flatten\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[0;32m--> 496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    497\u001b[0m     k: {\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: v\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: _tobytes(v, k),\n\u001b[1;32m    501\u001b[0m     }\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    503\u001b[0m }\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/safetensors/torch.py:500\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    497\u001b[0m     k: {\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: v\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m--> 500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    501\u001b[0m     }\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    503\u001b[0m }\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/safetensors/torch.py:414\u001b[0m, in \u001b[0;36m_tobytes\u001b[0;34m(tensor, name)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to save a sparse tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` which this library does not support.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You can make it a dense tensor before saving with `.to_dense()` but be aware this might\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make a much larger file than needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mis_contiguous():\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` which is not allowed. It either means you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms recommended to save\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pack it before saving.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     )\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: You are trying to save a non contiguous tensor: `transformer.encoder.layer.0.attention.self.query.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving."],"ename":"ValueError","evalue":"You are trying to save a non contiguous tensor: `transformer.encoder.layer.0.attention.self.query.weight` which is not allowed. It either means you are trying to save tensors which are reference of each other in which case it's recommended to save only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to pack it before saving.","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"# Train Bert Large uncased\ntrainer_bert_large = train_and_save_model('bert-base-uncased', 'bert-base-uncased-model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T15:52:17.920278Z","iopub.execute_input":"2024-11-17T15:52:17.920578Z","iopub.status.idle":"2024-11-17T16:30:36.122369Z","shell.execute_reply.started":"2024-11-17T15:52:17.920544Z","shell.execute_reply":"2024-11-17T16:30:36.121447Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43c2feec2c434d80ae301c4edaeedb16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0816f42cdda9414682e2b3ba30350ecd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71aa5f0bbe5e490caf6ec35edf445545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df37f42ab5dd4b259a64148b711e721d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"936c38e0aeff41d896e9e9b1c1b6c2a1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2392' max='2392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2392/2392 37:41, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Category Acc</th>\n      <th>Product Category Acc</th>\n      <th>Hazard Acc</th>\n      <th>Product Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>15.922500</td>\n      <td>15.637586</td>\n      <td>0.591290</td>\n      <td>0.287270</td>\n      <td>0.175879</td>\n      <td>0.005025</td>\n      <td>0.264866</td>\n      <td>0.172268</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>13.125700</td>\n      <td>12.940312</td>\n      <td>0.847571</td>\n      <td>0.340034</td>\n      <td>0.386097</td>\n      <td>0.041876</td>\n      <td>0.403894</td>\n      <td>0.316606</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>11.866700</td>\n      <td>11.654268</td>\n      <td>0.877722</td>\n      <td>0.391960</td>\n      <td>0.515075</td>\n      <td>0.057789</td>\n      <td>0.460637</td>\n      <td>0.387770</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>10.838800</td>\n      <td>11.045493</td>\n      <td>0.882747</td>\n      <td>0.469849</td>\n      <td>0.586265</td>\n      <td>0.070352</td>\n      <td>0.502303</td>\n      <td>0.437414</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>10.267000</td>\n      <td>10.678938</td>\n      <td>0.888610</td>\n      <td>0.498325</td>\n      <td>0.623116</td>\n      <td>0.071189</td>\n      <td>0.520310</td>\n      <td>0.459726</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>9.716600</td>\n      <td>10.452283</td>\n      <td>0.887772</td>\n      <td>0.509213</td>\n      <td>0.637353</td>\n      <td>0.075377</td>\n      <td>0.527429</td>\n      <td>0.467604</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>9.482000</td>\n      <td>10.307032</td>\n      <td>0.890285</td>\n      <td>0.520938</td>\n      <td>0.643216</td>\n      <td>0.078727</td>\n      <td>0.533291</td>\n      <td>0.476083</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>10.169200</td>\n      <td>10.258956</td>\n      <td>0.889447</td>\n      <td>0.532663</td>\n      <td>0.646566</td>\n      <td>0.079564</td>\n      <td>0.537060</td>\n      <td>0.479919</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:21]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for bert-base-uncased:\n{'eval_loss': 10.258955955505371, 'eval_hazard_category_acc': 0.8894472361809045, 'eval_product_category_acc': 0.5326633165829145, 'eval_hazard_acc': 0.6465661641541038, 'eval_product_acc': 0.07956448911222781, 'eval_avg_accuracy': 0.5370603015075378, 'eval_avg_f1': 0.4799187051760568, 'eval_runtime': 21.6337, 'eval_samples_per_second': 55.192, 'eval_steps_per_second': 3.467, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Train DeBERTa Large\ntrainer_deberta_large = train_and_save_model('microsoft/deberta-base', 'deberta-base-model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T16:30:36.127287Z","iopub.execute_input":"2024-11-17T16:30:36.127629Z","iopub.status.idle":"2024-11-17T17:33:49.177647Z","shell.execute_reply.started":"2024-11-17T16:30:36.127594Z","shell.execute_reply":"2024-11-17T17:33:49.176612Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d325db961be140a0ab3e418a7faceb91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94636e65880f426d9b1084bcf6a578b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"debceb895afb41d1bed59741eebd9830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2796c35de71b42bfbba055443ae0736f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8278057618314c2881e7b30d2e7759e6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2392' max='2392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2392/2392 1:02:21, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Category Acc</th>\n      <th>Product Category Acc</th>\n      <th>Hazard Acc</th>\n      <th>Product Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>13.740400</td>\n      <td>12.877097</td>\n      <td>0.762982</td>\n      <td>0.336683</td>\n      <td>0.380235</td>\n      <td>0.032663</td>\n      <td>0.378141</td>\n      <td>0.291854</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>10.809600</td>\n      <td>10.785297</td>\n      <td>0.871859</td>\n      <td>0.386097</td>\n      <td>0.634841</td>\n      <td>0.045226</td>\n      <td>0.484506</td>\n      <td>0.424018</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>9.927000</td>\n      <td>9.669526</td>\n      <td>0.889447</td>\n      <td>0.505863</td>\n      <td>0.691792</td>\n      <td>0.092127</td>\n      <td>0.544807</td>\n      <td>0.491236</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>8.646000</td>\n      <td>9.033434</td>\n      <td>0.896985</td>\n      <td>0.610553</td>\n      <td>0.714405</td>\n      <td>0.134003</td>\n      <td>0.588987</td>\n      <td>0.541592</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>8.105700</td>\n      <td>8.571469</td>\n      <td>0.907035</td>\n      <td>0.649079</td>\n      <td>0.732831</td>\n      <td>0.164992</td>\n      <td>0.613484</td>\n      <td>0.567313</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>7.619100</td>\n      <td>8.331431</td>\n      <td>0.907873</td>\n      <td>0.675042</td>\n      <td>0.742044</td>\n      <td>0.180067</td>\n      <td>0.626256</td>\n      <td>0.579581</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>7.134500</td>\n      <td>8.186049</td>\n      <td>0.907873</td>\n      <td>0.690117</td>\n      <td>0.744556</td>\n      <td>0.189280</td>\n      <td>0.632956</td>\n      <td>0.587398</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>7.699700</td>\n      <td>8.124866</td>\n      <td>0.907873</td>\n      <td>0.692630</td>\n      <td>0.747069</td>\n      <td>0.194305</td>\n      <td>0.635469</td>\n      <td>0.590822</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:38]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for microsoft/deberta-base:\n{'eval_loss': 8.124865531921387, 'eval_hazard_category_acc': 0.9078726968174204, 'eval_product_category_acc': 0.6926298157453936, 'eval_hazard_acc': 0.7470686767169179, 'eval_product_acc': 0.19430485762144054, 'eval_avg_accuracy': 0.635469011725293, 'eval_avg_f1': 0.5908215708833717, 'eval_runtime': 39.1956, 'eval_samples_per_second': 30.463, 'eval_steps_per_second': 1.913, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Train XLM-Roberta Large\ntrainer_xlm_roberta_large = train_and_save_model('xlm-roberta-base', 'xlm-roberta-base-model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T17:34:43.897869Z","iopub.execute_input":"2024-11-17T17:34:43.898625Z","iopub.status.idle":"2024-11-17T18:20:33.234286Z","shell.execute_reply.started":"2024-11-17T17:34:43.898562Z","shell.execute_reply":"2024-11-17T18:20:33.233452Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab32656e91974461b67f0f595871008b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce59156472e04733b308387288edd940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"138502d3985f4269972552af59fd84af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6425d4e67f4e109f0770b9abf446d0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"095a72c36e984dda9d0aae784af8b965"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2392' max='2392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2392/2392 45:07, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Category Acc</th>\n      <th>Product Category Acc</th>\n      <th>Hazard Acc</th>\n      <th>Product Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>15.786600</td>\n      <td>15.343690</td>\n      <td>0.554439</td>\n      <td>0.287270</td>\n      <td>0.131491</td>\n      <td>0.030988</td>\n      <td>0.251047</td>\n      <td>0.150584</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>13.034000</td>\n      <td>12.950892</td>\n      <td>0.836683</td>\n      <td>0.319933</td>\n      <td>0.396985</td>\n      <td>0.030988</td>\n      <td>0.396147</td>\n      <td>0.306388</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>12.221800</td>\n      <td>11.950312</td>\n      <td>0.856784</td>\n      <td>0.340871</td>\n      <td>0.458124</td>\n      <td>0.029313</td>\n      <td>0.421273</td>\n      <td>0.349680</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>11.202600</td>\n      <td>11.433931</td>\n      <td>0.877722</td>\n      <td>0.350921</td>\n      <td>0.486600</td>\n      <td>0.031826</td>\n      <td>0.436767</td>\n      <td>0.364580</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>10.687300</td>\n      <td>11.046580</td>\n      <td>0.885260</td>\n      <td>0.378559</td>\n      <td>0.585427</td>\n      <td>0.036013</td>\n      <td>0.471315</td>\n      <td>0.406737</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>10.206100</td>\n      <td>10.776897</td>\n      <td>0.891960</td>\n      <td>0.394472</td>\n      <td>0.610553</td>\n      <td>0.041876</td>\n      <td>0.484715</td>\n      <td>0.419708</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>10.051200</td>\n      <td>10.634160</td>\n      <td>0.896147</td>\n      <td>0.419598</td>\n      <td>0.622278</td>\n      <td>0.041876</td>\n      <td>0.494975</td>\n      <td>0.434758</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>10.607500</td>\n      <td>10.580544</td>\n      <td>0.895310</td>\n      <td>0.422948</td>\n      <td>0.623116</td>\n      <td>0.044389</td>\n      <td>0.496441</td>\n      <td>0.436061</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:23]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for xlm-roberta-base:\n{'eval_loss': 10.580544471740723, 'eval_hazard_category_acc': 0.8953098827470687, 'eval_product_category_acc': 0.42294807370184256, 'eval_hazard_acc': 0.6231155778894473, 'eval_product_acc': 0.04438860971524288, 'eval_avg_accuracy': 0.4964405360134003, 'eval_avg_f1': 0.4360610710129702, 'eval_runtime': 24.3299, 'eval_samples_per_second': 49.075, 'eval_steps_per_second': 3.083, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"trainer_distilbert_base = train_and_save_model('distilbert-base-uncased', 'distilbert-base-uncased-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:45:11.936054Z","iopub.execute_input":"2024-11-18T05:45:11.936835Z","iopub.status.idle":"2024-11-18T06:06:13.632146Z","shell.execute_reply.started":"2024-11-18T05:45:11.936795Z","shell.execute_reply":"2024-11-18T06:06:13.631146Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2392' max='2392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2392/2392 20:43, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Category Acc</th>\n      <th>Product Category Acc</th>\n      <th>Hazard Acc</th>\n      <th>Product Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>14.949300</td>\n      <td>14.557254</td>\n      <td>0.617253</td>\n      <td>0.288107</td>\n      <td>0.258794</td>\n      <td>0.030988</td>\n      <td>0.298786</td>\n      <td>0.192012</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>11.979800</td>\n      <td>11.921219</td>\n      <td>0.855946</td>\n      <td>0.355946</td>\n      <td>0.562814</td>\n      <td>0.036851</td>\n      <td>0.452889</td>\n      <td>0.381099</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>11.026100</td>\n      <td>10.794368</td>\n      <td>0.869347</td>\n      <td>0.447236</td>\n      <td>0.656616</td>\n      <td>0.065327</td>\n      <td>0.509631</td>\n      <td>0.445182</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>9.912300</td>\n      <td>10.284127</td>\n      <td>0.881910</td>\n      <td>0.503350</td>\n      <td>0.680067</td>\n      <td>0.088777</td>\n      <td>0.538526</td>\n      <td>0.481421</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>9.521600</td>\n      <td>9.956043</td>\n      <td>0.881072</td>\n      <td>0.530988</td>\n      <td>0.689280</td>\n      <td>0.103853</td>\n      <td>0.551298</td>\n      <td>0.494834</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>8.980500</td>\n      <td>9.737551</td>\n      <td>0.880235</td>\n      <td>0.551926</td>\n      <td>0.695142</td>\n      <td>0.120603</td>\n      <td>0.561977</td>\n      <td>0.506725</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>8.700500</td>\n      <td>9.614624</td>\n      <td>0.880235</td>\n      <td>0.565327</td>\n      <td>0.700168</td>\n      <td>0.123116</td>\n      <td>0.567211</td>\n      <td>0.514524</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>9.332500</td>\n      <td>9.571804</td>\n      <td>0.880235</td>\n      <td>0.571189</td>\n      <td>0.701843</td>\n      <td>0.126466</td>\n      <td>0.569933</td>\n      <td>0.516511</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:12]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for distilbert-base-uncased:\n{'eval_loss': 9.57180404663086, 'eval_hazard_category_acc': 0.8802345058626466, 'eval_product_category_acc': 0.5711892797319933, 'eval_hazard_acc': 0.7018425460636516, 'eval_product_acc': 0.12646566164154105, 'eval_avg_accuracy': 0.5699329983249581, 'eval_avg_f1': 0.5165108017850886, 'eval_runtime': 12.4311, 'eval_samples_per_second': 96.049, 'eval_steps_per_second': 6.033, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Import libraries\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, Trainer, TrainingArguments, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:36:41.774625Z","iopub.execute_input":"2024-11-17T18:36:41.775428Z","iopub.status.idle":"2024-11-17T18:37:04.248679Z","shell.execute_reply.started":"2024-11-17T18:36:41.775385Z","shell.execute_reply":"2024-11-17T18:37:04.247681Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Load the test data (validation data is actually the test data)\ntest_df = pd.read_csv('/kaggle/working/final_cleaned_validation.csv')\n\n# Drop unnecessary columns\ntest_df = test_df[['text']]\ntest_texts = test_df['text'].tolist()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-18T05:30:33.248433Z","iopub.execute_input":"2024-11-18T05:30:33.248832Z","iopub.status.idle":"2024-11-18T05:30:33.275137Z","shell.execute_reply.started":"2024-11-18T05:30:33.248795Z","shell.execute_reply":"2024-11-18T05:30:33.274397Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!pip install safetensors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:30:34.533175Z","iopub.execute_input":"2024-11-18T05:30:34.533551Z","iopub.status.idle":"2024-11-18T05:30:47.917661Z","shell.execute_reply.started":"2024-11-18T05:30:34.533516Z","shell.execute_reply":"2024-11-18T05:30:47.916694Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (0.4.5)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nfrom safetensors.torch import load_file\nimport numpy as np\nimport torch\n\ndef get_model_logits(model_name, model_dir, test_texts, batch_size=8):\n    # Initialize the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Tokenize the test data\n    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n\n    # Convert tokenized inputs to a TensorDataset\n    test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'])\n\n    # Use DataLoader to load the data in batches\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # Define the number of labels\n    num_labels_dict = {\n        'hazard_category': num_hazard_category_labels,\n        'product_category': num_product_category_labels,\n        'hazard': num_hazard_labels,\n        'product': num_product_labels\n    }\n\n    # Initialize the model\n    model = TransformerForFoodHazardClassification(model_name, num_labels_dict)\n\n    # Load the model state dict from model.safetensors\n    state_dict = load_file(f\"{model_dir}/model.safetensors\")\n    model.load_state_dict(state_dict)\n\n    # Move model to the GPUs (using DataParallel for multiple GPUs)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Default to cuda:0\n    model = torch.nn.DataParallel(model, device_ids=[0, 1])  # Use both GPU 0 and GPU 1\n    model.to(device)\n\n    # Initialize dictionaries to accumulate logits\n    all_hazard_category_logits = []\n    all_product_category_logits = []\n    all_hazard_logits = []\n    all_product_logits = []\n\n    with torch.no_grad():\n        model.eval()\n        for batch in test_dataloader:\n            input_ids, attention_mask = [b.to(device) for b in batch]\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n            # Unpack logits and move them to CPU\n            hazard_category_logits, product_category_logits, hazard_logits, product_logits = outputs\n            all_hazard_category_logits.append(hazard_category_logits.cpu().numpy())\n            all_product_category_logits.append(product_category_logits.cpu().numpy())\n            all_hazard_logits.append(hazard_logits.cpu().numpy())\n            all_product_logits.append(product_logits.cpu().numpy())\n\n    # Concatenate all logits from batches along the batch dimension (axis=0)\n    hazard_category_logits_concat = np.concatenate(all_hazard_category_logits, axis=0)\n    product_category_logits_concat = np.concatenate(all_product_category_logits, axis=0)\n    hazard_logits_concat = np.concatenate(all_hazard_logits, axis=0)\n    product_logits_concat = np.concatenate(all_product_logits, axis=0)\n\n    # Free GPU memory by deleting model and test_encodings\n    del model\n    del test_encodings\n\n    # Clear CUDA cache\n    torch.cuda.empty_cache()\n\n    # Return logits as a structured dictionary\n    return {\n        'hazard_category': hazard_category_logits_concat,\n        'product_category': product_category_logits_concat,\n        'hazard': hazard_logits_concat,\n        'product': product_logits_concat\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:31:14.316958Z","iopub.execute_input":"2024-11-18T05:31:14.317612Z","iopub.status.idle":"2024-11-18T05:31:14.331586Z","shell.execute_reply.started":"2024-11-18T05:31:14.317571Z","shell.execute_reply":"2024-11-18T05:31:14.330695Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Get logits from Bert Large\nbert_large_logits = get_model_logits('bert-base-uncased', 'bert-base-uncased-model', test_texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:31:15.307734Z","iopub.execute_input":"2024-11-18T05:31:15.308317Z","iopub.status.idle":"2024-11-18T05:31:32.486246Z","shell.execute_reply.started":"2024-11-18T05:31:15.308279Z","shell.execute_reply":"2024-11-18T05:31:32.485403Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52f50ac619264ea89f9d20f89528d089"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db6d0f9d1d5348fcbc159539dd64e480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b45ad0bbc7469499e6e3f09d0497f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dd4e5daa92943f7bc9e8324b70934c3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7576188d9cc44bcdbf6bc7d3e5052aec"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# After evaluating the model\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:31:32.599348Z","iopub.execute_input":"2024-11-18T05:31:32.599961Z","iopub.status.idle":"2024-11-18T05:31:32.604824Z","shell.execute_reply.started":"2024-11-18T05:31:32.599916Z","shell.execute_reply":"2024-11-18T05:31:32.603833Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Get logits from DeBERTa Large\ndeberta_large_logits = get_model_logits('microsoft/deberta-base', 'deberta-base-model', test_texts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:31:32.606483Z","iopub.execute_input":"2024-11-18T05:31:32.606841Z","iopub.status.idle":"2024-11-18T05:31:57.977458Z","shell.execute_reply.started":"2024-11-18T05:31:32.606799Z","shell.execute_reply":"2024-11-18T05:31:57.976654Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71a4b580ab484a32aa31820c508b61e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29ccd020b94647fba728a620918111c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8f653c944b942138a1ad5d7e94ee50f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18af7c0d0aa84e4a8ac6bf6719d60afd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00d9deb0764144018c142555b5d1cf76"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# After evaluating the model\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:31:58.006917Z","iopub.execute_input":"2024-11-18T05:31:58.007178Z","iopub.status.idle":"2024-11-18T05:31:58.011239Z","shell.execute_reply.started":"2024-11-18T05:31:58.007148Z","shell.execute_reply":"2024-11-18T05:31:58.010414Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Get logits from XLM-Roberta Large\nxlm_roberta_large_logits = get_model_logits('xlm-roberta-base', 'xlm-roberta-base-model', test_texts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:31:58.013766Z","iopub.execute_input":"2024-11-18T05:31:58.014069Z","iopub.status.idle":"2024-11-18T05:32:33.422858Z","shell.execute_reply.started":"2024-11-18T05:31:58.014036Z","shell.execute_reply":"2024-11-18T05:32:33.421974Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f303329d65545719ccbda9fadda1eeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f510201563d049a287869be2e48f539e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3576371ed80f49069796ec6bb2aece27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"307e4d5079194ee5aa338535ed6fe139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acb850cd88f941afac2379a3f1b03b80"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# After evaluating the model\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:06:20.759091Z","iopub.execute_input":"2024-11-18T06:06:20.759961Z","iopub.status.idle":"2024-11-18T06:06:20.805582Z","shell.execute_reply.started":"2024-11-18T06:06:20.759908Z","shell.execute_reply":"2024-11-18T06:06:20.804569Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"distilbert_base_logits = get_model_logits('distilbert-base-uncased', 'distilbert-base-uncased-model', test_texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:06:22.931435Z","iopub.execute_input":"2024-11-18T06:06:22.932127Z","iopub.status.idle":"2024-11-18T06:06:30.262752Z","shell.execute_reply.started":"2024-11-18T06:06:22.932087Z","shell.execute_reply":"2024-11-18T06:06:30.261716Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# After evaluating the model\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:06:32.114499Z","iopub.execute_input":"2024-11-18T06:06:32.114914Z","iopub.status.idle":"2024-11-18T06:06:32.119404Z","shell.execute_reply.started":"2024-11-18T06:06:32.114873Z","shell.execute_reply":"2024-11-18T06:06:32.118463Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Average the logits\nhazard_category_logits_avg = (distilbert_base_logits['hazard_category'] + deberta_large_logits['hazard_category'] + bert_large_logits['hazard_category']) / 3\nproduct_category_logits_avg = (distilbert_base_logits['product_category'] + deberta_large_logits['product_category'] + bert_large_logits['product_category']) / 3\nhazard_logits_avg = (distilbert_base_logits['hazard'] + deberta_large_logits['hazard'] + bert_large_logits['hazard']) / 3\nproduct_logits_avg = (distilbert_base_logits['product'] + deberta_large_logits['product'] + bert_large_logits['product']) / 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:06:33.112737Z","iopub.execute_input":"2024-11-18T06:06:33.113083Z","iopub.status.idle":"2024-11-18T06:06:33.121191Z","shell.execute_reply.started":"2024-11-18T06:06:33.113049Z","shell.execute_reply":"2024-11-18T06:06:33.120272Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Get predicted labels\nhazard_category_preds = np.argmax(hazard_category_logits_avg, axis=1)\nproduct_category_preds = np.argmax(product_category_logits_avg, axis=1)\nhazard_preds = np.argmax(hazard_logits_avg, axis=1)\nproduct_preds = np.argmax(product_logits_avg, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:06:37.367470Z","iopub.execute_input":"2024-11-18T06:06:37.368217Z","iopub.status.idle":"2024-11-18T06:06:37.374524Z","shell.execute_reply.started":"2024-11-18T06:06:37.368173Z","shell.execute_reply":"2024-11-18T06:06:37.373302Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"print(hazard_category_preds)\nprint(hazard_category_encoder.classes_)  # Print the classes the encoder was trained on\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:06:38.474230Z","iopub.execute_input":"2024-11-18T06:06:38.474571Z","iopub.status.idle":"2024-11-18T06:06:38.481894Z","shell.execute_reply.started":"2024-11-18T06:06:38.474537Z","shell.execute_reply":"2024-11-18T06:06:38.480948Z"}},"outputs":[{"name":"stdout","text":"[1 1 1 0 4 4 1 9 4 1 4 9 2 2 4 2 0 1 1 2 1 0 4 0 0 4 0 1 0 0 1 1 0 2 0 4 1\n 4 1 0 0 9 1 5 4 0 1 0 0 4 4 4 5 4 4 0 0 1 2 4 4 1 0 1 1 1 1 1 1 4 0 5 0 1\n 1 1 0 0 1 0 4 1 0 2 1 1 0 0 1 1 1 0 0 4 0 0 4 1 1 0 1 1 1 1 0 1 0 9 0 0 4\n 1 0 0 1 0 2 1 1 1 1 1 1 1 1 0 0 2 5 4 1 1 1 0 0 0 0 1 0 0 2 5 1 0 5 0 5 4\n 0 1 1 1 0 0 5 0 0 0 0 0 0 1 0 1 1 2 2 0 1 1 4 0 1 1 0 5 0 1 0 5 4 1 1 1 1\n 4 5 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 4 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 5 0 1\n 0 0 4 4 5 1 1 1 0 1 4 0 5 0 1 1 0 0 1 1 7 1 1 0 0 0 1 1 4 1 0 1 1 4 2 2 0\n 1 1 1 1 1 1 1 1 1 0 4 0 0 0 4 5 1 5 0 0 4 0 0 0 0 0 1 0 0 0 1 1 0 0 5 0 4\n 0 0 0 5 0 1 0 1 4 0 1 4 1 2 1 0 1 0 0 1 1 0 1 4 0 9 4 4 5 2 1 5 9 0 5 0 0\n 1 1 1 0 1 0 4 0 5 2 0 0 1 0 0 1 0 0 0 4 1 0 0 4 0 1 4 4 0 0 1 0 0 5 0 0 1\n 0 4 1 0 1 1 0 1 0 0 1 4 0 5 0 0 4 1 2 0 9 0 0 0 4 0 0 0 4 1 0 0 1 0 0 1 1\n 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 4 4 0 0 1 0 0 5 0 0 0 0 0 1 1 0 0 1 2 5\n 5 1 0 5 0 0 1 1 0 0 1 1 1 0 1 0 0 1 4 0 0 1 9 2 4 1 1 2 1 0 0 0 1 2 4 0 0\n 0 4 1 0 0 0 4 0 2 1 0 1 1 0 0 0 0 0 0 0 1 4 8 0 1 1 0 0 1 0 2 1 0 2 0 5 4\n 1 5 1 1 2 1 5 2 1 0 1 0 0 1 1 0 1 0 5 1 4 1 2 2 2 0 1 4 9 1 0 0 0 1 1 1 0\n 0 1 2 1 2 0 0 4 0 0]\n['allergens' 'biological' 'chemical' 'food additives and flavourings'\n 'foreign bodies' 'fraud' 'migration' 'organoleptic aspects'\n 'other hazard' 'packaging defect']\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"print(hazard_category_logits_avg.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:06:39.656426Z","iopub.execute_input":"2024-11-18T06:06:39.657090Z","iopub.status.idle":"2024-11-18T06:06:39.661678Z","shell.execute_reply.started":"2024-11-18T06:06:39.657046Z","shell.execute_reply":"2024-11-18T06:06:39.660815Z"}},"outputs":[{"name":"stdout","text":"(565, 10)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# Decode the predicted labels using the label encoders\nhazard_category_labels = hazard_category_encoder.inverse_transform(hazard_category_preds)\nproduct_category_labels = product_category_encoder.inverse_transform(product_category_preds)\nhazard_labels = hazard_encoder.inverse_transform(hazard_preds)\nproduct_labels = product_encoder.inverse_transform(product_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:06:41.320728Z","iopub.execute_input":"2024-11-18T06:06:41.321103Z","iopub.status.idle":"2024-11-18T06:06:41.328856Z","shell.execute_reply.started":"2024-11-18T06:06:41.321068Z","shell.execute_reply":"2024-11-18T06:06:41.327870Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Create a DataFrame for the predictions\noutput_df = pd.DataFrame({\n    'hazard-category': hazard_category_labels,\n    'product-category': product_category_labels,\n    'hazard': hazard_labels,\n    'product': product_labels\n})\n\n# Save the output DataFrame to a CSV file\noutput_df.to_csv('test_predictions_ensemble.csv', index=False)\n\n# For subtask 1 (hazard-category and product-category)\nsubtask1_df = output_df[['hazard-category', 'product-category']]\nsubtask1_df.to_csv('subtask1_predictions_ensemble.csv', index=True)\n\n# For subtask 2 (hazard and product)\nsubtask2_df = output_df[['hazard', 'product']]\nsubtask2_df.to_csv('subtask2_predictions_ensemble.csv', index=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:06:41.983281Z","iopub.execute_input":"2024-11-18T06:06:41.983611Z","iopub.status.idle":"2024-11-18T06:06:41.999467Z","shell.execute_reply.started":"2024-11-18T06:06:41.983577Z","shell.execute_reply":"2024-11-18T06:06:41.998658Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Analyze the predictions\nprint(\"Hazard Category Predictions:\")\nprint(subtask1_df['hazard-category'].value_counts())\n\nprint(\"\\nProduct Category Predictions:\")\nprint(subtask1_df['product-category'].value_counts())\n\nprint(\"\\nHazard Predictions:\")\nprint(subtask2_df['hazard'].value_counts())\n\nprint(\"\\nProduct Predictions:\")\nprint(subtask2_df['product'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:06:43.289566Z","iopub.execute_input":"2024-11-18T06:06:43.290004Z","iopub.status.idle":"2024-11-18T06:06:43.302844Z","shell.execute_reply.started":"2024-11-18T06:06:43.289966Z","shell.execute_reply":"2024-11-18T06:06:43.301727Z"}},"outputs":[{"name":"stdout","text":"Hazard Category Predictions:\nhazard-category\nallergens               226\nbiological              200\nforeign bodies           64\nchemical                 32\nfraud                    32\npackaging defect          9\norganoleptic aspects      1\nother hazard              1\nName: count, dtype: int64\n\nProduct Category Predictions:\nproduct-category\nmeat, egg and dairy products                         179\ncereals and bakery products                          123\nfruits and vegetables                                 76\nnuts, nut products and seeds                          34\nprepared dishes and snacks                            29\nnon-alcoholic beverages                               23\nsoups, broths, sauces and condiments                  23\ncocoa and cocoa preparations, coffee and tea          20\nices and desserts                                     20\nseafood                                               20\ndietetic foods, food supplements, fortified foods      5\nherbs and spices                                       5\nalcoholic beverages                                    4\nconfectionery                                          4\nName: count, dtype: int64\n\nHazard Predictions:\nhazard\nlisteria monocytogenes                            75\nsalmonella                                        72\nmilk and products thereof                         71\nescherichia coli                                  39\npeanuts and products thereof                      37\nplastic fragment                                  36\neggs and products thereof                         34\ninspection issues                                 27\ncereals containing gluten and products thereof    26\nsoybeans and products thereof                     25\nother                                             23\nmetal fragment                                    20\nsulphur dioxide and sulphites                     13\nalmond                                            12\nglass fragment                                    12\nunauthorised substance ethylene oxide              8\nbulging packaging                                  7\nclostridium botulinum                              5\nsesame seeds and products thereof                  5\nmustard and products thereof                       4\nheavy metals                                       3\nmoulds                                             3\nspoilage                                           3\nfish and products thereof                          1\ntoxin                                              1\nnorovirus                                          1\nsulphur dioxide                                    1\ncashew                                             1\nName: count, dtype: int64\n\nProduct Predictions:\nproduct\nchicken based products       159\ncakes                         98\nice cream                     70\ncheese                        35\ncookies                       34\npistachio nuts                31\nsalmon                        17\nbeer                          16\nsalads                        16\nground beef                   13\ncoconuts                      10\nfood supplement                8\ndried apricots                 7\nsoup                           5\nbread                          5\nflour                          5\nchocolate bars                 5\nready to eat - cook meals      4\npeanuts                        4\napple juice                    3\njellies                        3\nsesame seeds                   2\napricot kernels                2\nmilk                           2\ncurry                          2\nfresh mushrooms                2\neggs                           1\npalm oil                       1\nsausage                        1\nsandwiches                     1\nonions                         1\ndates                          1\ndried plums                    1\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}