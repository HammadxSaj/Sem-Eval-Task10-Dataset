{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9941357,"sourceType":"datasetVersion","datasetId":6112362},{"sourceId":10061618,"sourceType":"datasetVersion","datasetId":6200554}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, Trainer, TrainingArguments, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:31.455239Z","iopub.execute_input":"2024-11-30T20:26:31.455779Z","iopub.status.idle":"2024-11-30T20:26:31.460527Z","shell.execute_reply.started":"2024-11-30T20:26:31.455749Z","shell.execute_reply":"2024-11-30T20:26:31.459732Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Download datasets\n!wget -P /kaggle/working -nc \"https://raw.githubusercontent.com/HammadxSaj/Sem-Eval-Task10-Dataset/refs/heads/main/final_cleaned_train.csv\"\n!wget -P /kaggle/working -nc \"https://raw.githubusercontent.com/HammadxSaj/Sem-Eval-Task10-Dataset/refs/heads/main/final_cleaned_validation.csv\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:54.822704Z","iopub.execute_input":"2024-11-30T20:26:54.822985Z","iopub.status.idle":"2024-11-30T20:26:56.838589Z","shell.execute_reply.started":"2024-11-30T20:26:54.822957Z","shell.execute_reply":"2024-11-30T20:26:56.837486Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"File '/kaggle/working/final_cleaned_train.csv' already there; not retrieving.\n\nFile '/kaggle/working/final_cleaned_validation.csv' already there; not retrieving.\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Load the training data\ndf = pd.read_csv('//kaggle/input/dataset/data.csv')\n\n# Inspect the dataframe\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:56.840697Z","iopub.execute_input":"2024-11-30T20:26:56.841003Z","iopub.status.idle":"2024-11-30T20:26:57.210655Z","shell.execute_reply.started":"2024-11-30T20:26:56.840973Z","shell.execute_reply":"2024-11-30T20:26:57.209736Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   year  month  day country                             title  \\\n0  1994      1    7      us  Recall Notification: FSIS-024-94   \n1  1994      3   10      us  Recall Notification: FSIS-033-94   \n2  1994      3   28      us  Recall Notification: FSIS-014-94   \n3  1994      4    3      us  Recall Notification: FSIS-009-94   \n4  1994      7    1      us  Recall Notification: FSIS-001-94   \n\n                                                text hazard-category  \\\n0  Case Number: 024-94   \\n            Date Opene...      biological   \n1  Case Number: 033-94   \\n            Date Opene...      biological   \n2  Case Number: 014-94   \\n            Date Opene...      biological   \n3  Case Number: 009-94   \\n            Date Opene...  foreign bodies   \n4  Case Number: 001-94   \\n            Date Opene...  foreign bodies   \n\n               product-category                  hazard  \\\n0  meat, egg and dairy products  listeria monocytogenes   \n1  meat, egg and dairy products            listeria spp   \n2  meat, egg and dairy products  listeria monocytogenes   \n3  meat, egg and dairy products        plastic fragment   \n4  meat, egg and dairy products        plastic fragment   \n\n                       product  \\\n0               smoked sausage   \n1                      sausage   \n2                   ham slices   \n3  thermal processed pork meat   \n4               chicken breast   \n\n                                       combined_text  label  \n0  Recall Notification: FSIS-024-94 Case Number: ...     55  \n1  Recall Notification: FSIS-033-94 Case Number: ...     56  \n2  Recall Notification: FSIS-014-94 Case Number: ...     55  \n3  Recall Notification: FSIS-009-94 Case Number: ...     90  \n4  Recall Notification: FSIS-001-94 Case Number: ...     90  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>country</th>\n      <th>title</th>\n      <th>text</th>\n      <th>hazard-category</th>\n      <th>product-category</th>\n      <th>hazard</th>\n      <th>product</th>\n      <th>combined_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1994</td>\n      <td>1</td>\n      <td>7</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-024-94</td>\n      <td>Case Number: 024-94   \\n            Date Opene...</td>\n      <td>biological</td>\n      <td>meat, egg and dairy products</td>\n      <td>listeria monocytogenes</td>\n      <td>smoked sausage</td>\n      <td>Recall Notification: FSIS-024-94 Case Number: ...</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1994</td>\n      <td>3</td>\n      <td>10</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-033-94</td>\n      <td>Case Number: 033-94   \\n            Date Opene...</td>\n      <td>biological</td>\n      <td>meat, egg and dairy products</td>\n      <td>listeria spp</td>\n      <td>sausage</td>\n      <td>Recall Notification: FSIS-033-94 Case Number: ...</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1994</td>\n      <td>3</td>\n      <td>28</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-014-94</td>\n      <td>Case Number: 014-94   \\n            Date Opene...</td>\n      <td>biological</td>\n      <td>meat, egg and dairy products</td>\n      <td>listeria monocytogenes</td>\n      <td>ham slices</td>\n      <td>Recall Notification: FSIS-014-94 Case Number: ...</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1994</td>\n      <td>4</td>\n      <td>3</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-009-94</td>\n      <td>Case Number: 009-94   \\n            Date Opene...</td>\n      <td>foreign bodies</td>\n      <td>meat, egg and dairy products</td>\n      <td>plastic fragment</td>\n      <td>thermal processed pork meat</td>\n      <td>Recall Notification: FSIS-009-94 Case Number: ...</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1994</td>\n      <td>7</td>\n      <td>1</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-001-94</td>\n      <td>Case Number: 001-94   \\n            Date Opene...</td>\n      <td>foreign bodies</td>\n      <td>meat, egg and dairy products</td>\n      <td>plastic fragment</td>\n      <td>chicken breast</td>\n      <td>Recall Notification: FSIS-001-94 Case Number: ...</td>\n      <td>90</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df['text'] = df['hazard-category'] + ' ' + df['product-category'] + ' ' + df['text']\n\n# Drop 'hazard-category' and 'product-category'\ndf = df.drop(columns=['hazard-category', 'product-category'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.212137Z","iopub.execute_input":"2024-11-30T20:26:57.212554Z","iopub.status.idle":"2024-11-30T20:26:57.240046Z","shell.execute_reply.started":"2024-11-30T20:26:57.212515Z","shell.execute_reply":"2024-11-30T20:26:57.239208Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.242477Z","iopub.execute_input":"2024-11-30T20:26:57.243216Z","iopub.status.idle":"2024-11-30T20:26:57.256297Z","shell.execute_reply.started":"2024-11-30T20:26:57.243176Z","shell.execute_reply":"2024-11-30T20:26:57.255373Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"       year  month  day country  \\\n0      1994      1    7      us   \n1      1994      3   10      us   \n2      1994      3   28      us   \n3      1994      4    3      us   \n4      1994      7    1      us   \n...     ...    ...  ...     ...   \n12308  2022      6    6      us   \n12309  2022      6   14      ie   \n12310  2022      6   23      us   \n12311  2022      7    4      hk   \n12312  2022      7   19      hk   \n\n                                                   title  \\\n0                       Recall Notification: FSIS-024-94   \n1                       Recall Notification: FSIS-033-94   \n2                       Recall Notification: FSIS-014-94   \n3                       Recall Notification: FSIS-009-94   \n4                       Recall Notification: FSIS-001-94   \n...                                                  ...   \n12308                                              False   \n12309                                              False   \n12310  Paraphrase: Daily Harvest Issues Voluntary Rec...   \n12311                                              False   \n12312                                              False   \n\n                                                    text  \\\n0      biological meat, egg and dairy products Case N...   \n1      biological meat, egg and dairy products Case N...   \n2      biological meat, egg and dairy products Case N...   \n3      foreign bodies meat, egg and dairy products Ca...   \n4      foreign bodies meat, egg and dairy products Ca...   \n...                                                  ...   \n12308  allergens confectionery phrase: NAPERVILLE, Il...   \n12309  allergens cocoa and cocoa preparations, coffee...   \n12310  biological fruits and vegetables . French Lent...   \n12311  biological meat, egg and dairy products CFS fo...   \n12312  chemical other food product / mixed CFS urges ...   \n\n                                      hazard                          product  \\\n0                     listeria monocytogenes                   smoked sausage   \n1                               listeria spp                          sausage   \n2                     listeria monocytogenes                       ham slices   \n3                           plastic fragment      thermal processed pork meat   \n4                           plastic fragment                   chicken breast   \n...                                      ...                              ...   \n12308              milk and products thereof                     sprinkle mix   \n12309                                 almond  chocolate spread with hazelnuts   \n12310                                  other                      frozen leek   \n12311                                  virus            bovine meat and offal   \n12312  unauthorised substance ethylene oxide                    ramen noodles   \n\n                                           combined_text  label  \n0      Recall Notification: FSIS-024-94 Case Number: ...     55  \n1      Recall Notification: FSIS-033-94 Case Number: ...     56  \n2      Recall Notification: FSIS-014-94 Case Number: ...     55  \n3      Recall Notification: FSIS-009-94 Case Number: ...     90  \n4      Recall Notification: FSIS-001-94 Case Number: ...     90  \n...                                                  ...    ...  \n12308  False phrase: NAPERVILLE, Ill., June 2, 2022 (...     59  \n12309  False : Undeclared Almond in Batches of SPAR S...      5  \n12310  Paraphrase: Daily Harvest Issues Voluntary Rec...     73  \n12311  False CFS follows up on imported frozen beef a...    125  \n12312  False CFS urges public not to consume kind of ...    119  \n\n[12313 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>country</th>\n      <th>title</th>\n      <th>text</th>\n      <th>hazard</th>\n      <th>product</th>\n      <th>combined_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1994</td>\n      <td>1</td>\n      <td>7</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-024-94</td>\n      <td>biological meat, egg and dairy products Case N...</td>\n      <td>listeria monocytogenes</td>\n      <td>smoked sausage</td>\n      <td>Recall Notification: FSIS-024-94 Case Number: ...</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1994</td>\n      <td>3</td>\n      <td>10</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-033-94</td>\n      <td>biological meat, egg and dairy products Case N...</td>\n      <td>listeria spp</td>\n      <td>sausage</td>\n      <td>Recall Notification: FSIS-033-94 Case Number: ...</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1994</td>\n      <td>3</td>\n      <td>28</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-014-94</td>\n      <td>biological meat, egg and dairy products Case N...</td>\n      <td>listeria monocytogenes</td>\n      <td>ham slices</td>\n      <td>Recall Notification: FSIS-014-94 Case Number: ...</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1994</td>\n      <td>4</td>\n      <td>3</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-009-94</td>\n      <td>foreign bodies meat, egg and dairy products Ca...</td>\n      <td>plastic fragment</td>\n      <td>thermal processed pork meat</td>\n      <td>Recall Notification: FSIS-009-94 Case Number: ...</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1994</td>\n      <td>7</td>\n      <td>1</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-001-94</td>\n      <td>foreign bodies meat, egg and dairy products Ca...</td>\n      <td>plastic fragment</td>\n      <td>chicken breast</td>\n      <td>Recall Notification: FSIS-001-94 Case Number: ...</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12308</th>\n      <td>2022</td>\n      <td>6</td>\n      <td>6</td>\n      <td>us</td>\n      <td>False</td>\n      <td>allergens confectionery phrase: NAPERVILLE, Il...</td>\n      <td>milk and products thereof</td>\n      <td>sprinkle mix</td>\n      <td>False phrase: NAPERVILLE, Ill., June 2, 2022 (...</td>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>12309</th>\n      <td>2022</td>\n      <td>6</td>\n      <td>14</td>\n      <td>ie</td>\n      <td>False</td>\n      <td>allergens cocoa and cocoa preparations, coffee...</td>\n      <td>almond</td>\n      <td>chocolate spread with hazelnuts</td>\n      <td>False : Undeclared Almond in Batches of SPAR S...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>12310</th>\n      <td>2022</td>\n      <td>6</td>\n      <td>23</td>\n      <td>us</td>\n      <td>Paraphrase: Daily Harvest Issues Voluntary Rec...</td>\n      <td>biological fruits and vegetables . French Lent...</td>\n      <td>other</td>\n      <td>frozen leek</td>\n      <td>Paraphrase: Daily Harvest Issues Voluntary Rec...</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>12311</th>\n      <td>2022</td>\n      <td>7</td>\n      <td>4</td>\n      <td>hk</td>\n      <td>False</td>\n      <td>biological meat, egg and dairy products CFS fo...</td>\n      <td>virus</td>\n      <td>bovine meat and offal</td>\n      <td>False CFS follows up on imported frozen beef a...</td>\n      <td>125</td>\n    </tr>\n    <tr>\n      <th>12312</th>\n      <td>2022</td>\n      <td>7</td>\n      <td>19</td>\n      <td>hk</td>\n      <td>False</td>\n      <td>chemical other food product / mixed CFS urges ...</td>\n      <td>unauthorised substance ethylene oxide</td>\n      <td>ramen noodles</td>\n      <td>False CFS urges public not to consume kind of ...</td>\n      <td>119</td>\n    </tr>\n  </tbody>\n</table>\n<p>12313 rows × 10 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Data preprocessing\n\n# Drop unnecessary columns for training\ndf = df[['text', 'hazard', 'product']]\n\n# Drop rows with missing values\ndf.dropna(inplace=True)\n\n# # Initialize label encoders\n# hazard_category_encoder = LabelEncoder()\n# product_category_encoder = LabelEncoder()\nhazard_encoder = LabelEncoder()\nproduct_encoder = LabelEncoder()\n\n# Fit the encoders\n# hazard_category_encoder.fit(df['hazard-category'])\n# product_category_encoder.fit(df['product-category'])\nhazard_encoder.fit(df['hazard'])\nproduct_encoder.fit(df['product'])\n\n# Transform the labels\n# df['hazard-category'] = hazard_category_encoder.transform(df['hazard-category'])\n# df['product-category'] = product_category_encoder.transform(df['product-category'])\ndf['hazard'] = hazard_encoder.transform(df['hazard'])\ndf['product'] = product_encoder.transform(df['product'])\n\n# Split the data into train and validation sets\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(f\"Number of training samples: {len(train_df)}\")\nprint(f\"Number of validation samples: {len(val_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.257609Z","iopub.execute_input":"2024-11-30T20:26:57.257922Z","iopub.status.idle":"2024-11-30T20:26:57.284391Z","shell.execute_reply.started":"2024-11-30T20:26:57.257879Z","shell.execute_reply":"2024-11-30T20:26:57.283518Z"}},"outputs":[{"name":"stdout","text":"Number of training samples: 9850\nNumber of validation samples: 2463\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2321081200.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.dropna(inplace=True)\n/tmp/ipykernel_30/2321081200.py:24: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['hazard'] = hazard_encoder.transform(df['hazard'])\n/tmp/ipykernel_30/2321081200.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['product'] = product_encoder.transform(df['product'])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Define the FoodHazardDataset class\nclass FoodHazardDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, hazards, products):\n        self.encodings = encodings\n        # self.hazard_categories = hazard_categories\n        # self.product_categories = product_categories\n        self.hazards = hazards\n        self.products = products\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        # item['hazard_category_labels'] = torch.tensor(self.hazard_categories[idx])\n        # item['product_category_labels'] = torch.tensor(self.product_categories[idx])\n        item['hazard_labels'] = torch.tensor(self.hazards[idx])\n        item['product_labels'] = torch.tensor(self.products[idx])\n        return item\n\n    def __len__(self):\n        return len(self.hazards)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.285361Z","iopub.execute_input":"2024-11-30T20:26:57.285612Z","iopub.status.idle":"2024-11-30T20:26:57.291039Z","shell.execute_reply.started":"2024-11-30T20:26:57.285588Z","shell.execute_reply":"2024-11-30T20:26:57.290123Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Define the number of unique labels for each category\n# num_hazard_category_labels = len(hazard_category_encoder.classes_)\n# num_product_category_labels = len(product_category_encoder.classes_)\nnum_hazard_labels = len(hazard_encoder.classes_)\nnum_product_labels = len(product_encoder.classes_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.292091Z","iopub.execute_input":"2024-11-30T20:26:57.292452Z","iopub.status.idle":"2024-11-30T20:26:57.299406Z","shell.execute_reply.started":"2024-11-30T20:26:57.292409Z","shell.execute_reply":"2024-11-30T20:26:57.298413Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import AutoModel\nimport torch.nn as nn\n\nclass TransformerForFoodHazardClassification(nn.Module):\n    def __init__(self, model_name, num_labels_dict):\n        super().__init__()\n        self.transformer = AutoModel.from_pretrained(model_name)\n        # Uncomment the line below if you want to use dropout\n        # self.dropout = nn.Dropout(self.transformer.config.hidden_dropout_prob)\n\n        hidden_size = self.transformer.config.hidden_size\n\n        # Classifiers for the four labels\n        # self.hazard_category_classifier = nn.Linear(hidden_size, num_labels_dict['hazard_category'])\n        # self.product_category_classifier = nn.Linear(hidden_size, num_labels_dict['product_category'])\n        self.hazard_classifier = nn.Linear(hidden_size, num_labels_dict['hazard'])\n        self.product_classifier = nn.Linear(hidden_size, num_labels_dict['product'])\n\n        # Loss function\n        self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                hazard_labels=None, product_labels=None):\n        # Check if the model supports token_type_ids\n        if \"token_type_ids\" in self.transformer.forward.__code__.co_varnames:\n            outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        else:\n            # For DistilBERT and similar models that do not accept token_type_ids\n            outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n\n        # Select pooled output for models like BERT and DeBERTa, or use CLS token for others\n        if hasattr(outputs, 'pooler_output'):\n            pooled_output = outputs.pooler_output\n        else:\n            pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n\n        # Apply dropout if using\n        # pooled_output = self.dropout(pooled_output)\n\n        # Predict the four labels\n        # hazard_category_logits = self.hazard_category_classifier(pooled_output)\n        # product_category_logits = self.product_category_classifier(pooled_output)\n        hazard_logits = self.hazard_classifier(pooled_output)\n        product_logits = self.product_classifier(pooled_output)\n\n        loss = None\n        if hazard_labels is not None and product_labels is not None:\n            # Compute loss for each task\n            # hazard_category_loss = self.loss_fct(hazard_category_logits, hazard_category_labels)\n            # product_category_loss = self.loss_fct(product_category_logits, product_category_labels)\n            hazard_loss = self.loss_fct(hazard_logits, hazard_labels)\n            product_loss = self.loss_fct(product_logits, product_labels)\n\n            # Aggregate losses\n            loss = hazard_loss + product_loss\n\n        # Return the loss and logits\n        output = (hazard_logits, product_logits)\n        return ((loss,) + output) if loss is not None else output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.300580Z","iopub.execute_input":"2024-11-30T20:26:57.301197Z","iopub.status.idle":"2024-11-30T20:26:57.311646Z","shell.execute_reply.started":"2024-11-30T20:26:57.301143Z","shell.execute_reply":"2024-11-30T20:26:57.310894Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Define the compute_metrics function to calculate both accuracy and average F1 score across all labels\n\ndef compute_metrics(pred):\n\n    labels = pred.label_ids\n\n    preds = pred.predictions\n\n\n\n    # Unpack labels and predictions for each task\n\n    # hazard_category_labels = labels[0]\n\n    # product_category_labels = labels[1]\n\n    hazard_labels = labels[0]\n\n    product_labels = labels[1]\n\n\n\n    # hazard_category_preds = preds[0].argmax(-1)\n\n    # product_category_preds = preds[1].argmax(-1)\n\n    hazard_preds = preds[0].argmax(-1)\n\n    product_preds = preds[1].argmax(-1)\n\n\n\n    # Compute accuracy for each task (can be used separately if needed)\n\n    # hazard_category_acc = accuracy_score(hazard_category_labels, hazard_category_preds)\n\n    # product_category_acc = accuracy_score(product_category_labels, product_category_preds)\n\n    hazard_acc = accuracy_score(hazard_labels, hazard_preds)\n\n    product_acc = accuracy_score(product_labels, product_preds)\n\n\n\n    # Compute F1 score for each task\n\n    # hazard_category_f1 = f1_score(hazard_category_labels, hazard_category_preds, average='weighted')\n\n    # product_category_f1 = f1_score(product_category_labels, product_category_preds, average='weighted')\n\n    hazard_f1 = f1_score(hazard_labels, hazard_preds, average='weighted')\n\n    product_f1 = f1_score(product_labels, product_preds, average='weighted')\n\n\n\n    # Compute average F1 score across all tasks\n\n    avg_f1 = (hazard_f1 + product_f1) / 2\n\n\n\n    # Optionally, you can also compute average accuracy across tasks if needed\n\n    avg_acc = (hazard_acc + product_acc) / 2\n\n\n\n    # Return a dictionary with both accuracy and average F1 score\n\n    return {\n\n        'hazard_acc': hazard_acc,\n\n        'product_acc': product_acc,\n\n        'avg_accuracy': avg_acc,\n\n        'avg_f1': avg_f1\n\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.312510Z","iopub.execute_input":"2024-11-30T20:26:57.312794Z","iopub.status.idle":"2024-11-30T20:26:57.322895Z","shell.execute_reply.started":"2024-11-30T20:26:57.312755Z","shell.execute_reply":"2024-11-30T20:26:57.322204Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Define the data collator\ndef data_collator(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'hazard_labels': torch.tensor([x['hazard_labels'] for x in batch]),\n        'product_labels': torch.tensor([x['product_labels'] for x in batch]),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.325051Z","iopub.execute_input":"2024-11-30T20:26:57.325338Z","iopub.status.idle":"2024-11-30T20:26:57.333782Z","shell.execute_reply.started":"2024-11-30T20:26:57.325310Z","shell.execute_reply":"2024-11-30T20:26:57.333194Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Function to train and save a model\ndef train_and_save_model(model_name, output_dir):\n    \"\"\"\n    Trains and saves a model (only the final model after the last epoch).\n    \n    Args:\n    - model_name: the pre-trained model name or path.\n    - output_dir: directory to save the model\n    \"\"\"\n    # Initialize the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Tokenize the text data\n    train_texts = train_df['text'].tolist()\n    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n\n    val_texts = val_df['text'].tolist()\n    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n\n    # Prepare the datasets\n    train_dataset = FoodHazardDataset(\n        train_encodings,\n        # train_df['hazard-category'].tolist(),\n        # train_df['product-category'].tolist()\n        train_df['hazard'].tolist(),\n        train_df['product'].tolist()\n    )\n\n    val_dataset = FoodHazardDataset(\n        val_encodings,\n        # val_df['hazard-category'].tolist(),\n        # val_df['product-category'].tolist()\n        val_df['hazard'].tolist(),\n        val_df['product'].tolist()\n    )\n\n    # Define the number of labels\n    num_labels_dict = {\n        # 'hazard_category': num_hazard_category_labels,\n        # 'product_category': num_product_category_labels\n        'hazard': num_hazard_labels,\n        'product': num_product_labels\n    }\n\n    # Initialize the model\n    model = TransformerForFoodHazardClassification(model_name, num_labels_dict)\n\n    # Move the model to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    model.to(device)\n\n    # Training arguments\n\n    if model_name == \"microsoft/deberta-base\":\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=8,  # Train for 8 epochs\n            per_device_train_batch_size=8,  # Adjust based on your GPU memory\n            per_device_eval_batch_size=8,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"no\",  # Do not save after each epoch\n            logging_dir='./logs',\n            logging_steps=10,\n            warmup_steps=500,\n            weight_decay=0.01,\n            report_to=[]  # Disable W&B logging\n        )\n    else:\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=8,  # Train for 8 epochs\n            per_device_train_batch_size=16,  # Adjust based on your GPU memory\n            per_device_eval_batch_size=16,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"no\",  # Do not save after each epoch\n            logging_dir='./logs',\n            logging_steps=10,\n            warmup_steps=500,\n            weight_decay=0.01,\n            report_to=[]  # Disable W&B logging\n        )\n\n    # Initialize the Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics,\n        data_collator=data_collator,\n        optimizers=(AdamW(model.parameters(), lr=1e-5), None),\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Save the model only after the last epoch (epoch 8)\n    if model_name == 'allenai/scibert_scivocab_uncased':\n        # Save the model only after the last epoch (epoch 8)\n        state_dict = {k: v.contiguous() if isinstance(v, torch.Tensor) else v for k, v in model.state_dict().items()}\n        torch.save(state_dict, os.path.join(output_dir, \"scibert_weights\"))\n    else:\n        trainer.save_model(output_dir)\n\n    # Evaluate the model\n    eval_results = trainer.evaluate()\n    print(f\"Evaluation results for {model_name}:\")\n    print(eval_results)\n\n    # Clear GPU memory\n    del model\n    torch.cuda.empty_cache()\n\n    return eval_results  # Return evaluation results instead of the trainer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.334989Z","iopub.execute_input":"2024-11-30T20:26:57.335329Z","iopub.status.idle":"2024-11-30T20:26:57.346664Z","shell.execute_reply.started":"2024-11-30T20:26:57.335302Z","shell.execute_reply":"2024-11-30T20:26:57.345934Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.498601Z","iopub.execute_input":"2024-11-30T20:26:57.498873Z","iopub.status.idle":"2024-11-30T20:26:57.502911Z","shell.execute_reply.started":"2024-11-30T20:26:57.498847Z","shell.execute_reply":"2024-11-30T20:26:57.501903Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Train DeBERTa Large\ntrainer_deberta_base = train_and_save_model('microsoft/deberta-base', 'deberta-base-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T20:26:57.779563Z","iopub.execute_input":"2024-11-30T20:26:57.779855Z","iopub.status.idle":"2024-11-30T22:39:22.658238Z","shell.execute_reply.started":"2024-11-30T20:26:57.779829Z","shell.execute_reply":"2024-11-30T22:39:22.657223Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f09edec4ac4f1e916de0d1c2bbb7b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2749529ff2cd44ea83826dc035f217c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc0d56b6c4f94303867b2cc7f354d3d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"289bcb7d6a0b4c789ef7f8a7d0e2a89f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b329d0c021ab486196720fd919e66624"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4928' max='4928' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4928/4928 2:10:48, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Acc</th>\n      <th>Product Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>8.802900</td>\n      <td>8.570143</td>\n      <td>0.496143</td>\n      <td>0.049939</td>\n      <td>0.273041</td>\n      <td>0.214010</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>7.554000</td>\n      <td>7.097185</td>\n      <td>0.649614</td>\n      <td>0.142915</td>\n      <td>0.396265</td>\n      <td>0.340783</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>6.344100</td>\n      <td>6.216573</td>\n      <td>0.750305</td>\n      <td>0.220057</td>\n      <td>0.485181</td>\n      <td>0.435436</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5.703000</td>\n      <td>5.606031</td>\n      <td>0.812018</td>\n      <td>0.282582</td>\n      <td>0.547300</td>\n      <td>0.503038</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>5.144300</td>\n      <td>5.197421</td>\n      <td>0.830288</td>\n      <td>0.334551</td>\n      <td>0.582420</td>\n      <td>0.538545</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>4.954000</td>\n      <td>4.920473</td>\n      <td>0.846123</td>\n      <td>0.365814</td>\n      <td>0.605968</td>\n      <td>0.562844</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>4.367100</td>\n      <td>4.766680</td>\n      <td>0.855461</td>\n      <td>0.387333</td>\n      <td>0.621397</td>\n      <td>0.579406</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>4.672100</td>\n      <td>4.708915</td>\n      <td>0.854243</td>\n      <td>0.395047</td>\n      <td>0.624645</td>\n      <td>0.583338</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [154/154 01:20]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for microsoft/deberta-base:\n{'eval_loss': 4.708914756774902, 'eval_hazard_acc': 0.8542427933414535, 'eval_product_acc': 0.3950466910272026, 'eval_avg_accuracy': 0.624644742184328, 'eval_avg_f1': 0.5833376924914732, 'eval_runtime': 81.1817, 'eval_samples_per_second': 30.339, 'eval_steps_per_second': 1.897, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:39:22.660776Z","iopub.execute_input":"2024-11-30T22:39:22.661490Z","iopub.status.idle":"2024-11-30T22:39:22.712759Z","shell.execute_reply.started":"2024-11-30T22:39:22.661460Z","shell.execute_reply":"2024-11-30T22:39:22.711849Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"trainer_distilbert_base = train_and_save_model('distilbert-base-uncased', 'distilbert-base-uncased-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:39:22.713818Z","iopub.execute_input":"2024-11-30T22:39:22.714071Z","iopub.status.idle":"2024-11-30T23:19:57.392094Z","shell.execute_reply.started":"2024-11-30T22:39:22.714047Z","shell.execute_reply":"2024-11-30T23:19:57.391243Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a52b502a5a14a1687b932279b2d8bee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ad1bcac677417e9c3af3efaacbc2d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7411f15dd0e04b828273f6d323b87771"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a40b539c6d9444a886c1e2e9cb4dc93"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f84cbb72c54d5d8c9aeb7716a497e6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2464' max='2464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2464/2464 40:00, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Acc</th>\n      <th>Product Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>10.828000</td>\n      <td>10.793424</td>\n      <td>0.219245</td>\n      <td>0.020300</td>\n      <td>0.119773</td>\n      <td>0.050819</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>9.318000</td>\n      <td>9.020288</td>\n      <td>0.452294</td>\n      <td>0.066585</td>\n      <td>0.259440</td>\n      <td>0.197959</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>8.328900</td>\n      <td>8.102565</td>\n      <td>0.603735</td>\n      <td>0.133983</td>\n      <td>0.368859</td>\n      <td>0.314556</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>7.693600</td>\n      <td>7.579804</td>\n      <td>0.685749</td>\n      <td>0.181486</td>\n      <td>0.433618</td>\n      <td>0.381342</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>7.214300</td>\n      <td>7.238029</td>\n      <td>0.726756</td>\n      <td>0.209095</td>\n      <td>0.467925</td>\n      <td>0.417688</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>7.047800</td>\n      <td>7.010336</td>\n      <td>0.747868</td>\n      <td>0.215591</td>\n      <td>0.481730</td>\n      <td>0.432752</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>6.905900</td>\n      <td>6.883865</td>\n      <td>0.758831</td>\n      <td>0.227771</td>\n      <td>0.493301</td>\n      <td>0.444632</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>6.895200</td>\n      <td>6.841282</td>\n      <td>0.760455</td>\n      <td>0.232643</td>\n      <td>0.496549</td>\n      <td>0.447511</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [77/77 00:25]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for distilbert-base-uncased:\n{'eval_loss': 6.841282367706299, 'eval_hazard_acc': 0.76045473000406, 'eval_product_acc': 0.23264311814859928, 'eval_avg_accuracy': 0.49654892407632967, 'eval_avg_f1': 0.4475109004021072, 'eval_runtime': 25.9272, 'eval_samples_per_second': 94.997, 'eval_steps_per_second': 2.97, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T23:19:57.395342Z","iopub.execute_input":"2024-11-30T23:19:57.395612Z","iopub.status.idle":"2024-11-30T23:19:57.426973Z","shell.execute_reply.started":"2024-11-30T23:19:57.395585Z","shell.execute_reply":"2024-11-30T23:19:57.426062Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"trainer_scibert = train_and_save_model('allenai/scibert_scivocab_uncased', 'scibert_scivocab_uncased-model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T23:19:57.452777Z","iopub.execute_input":"2024-11-30T23:19:57.453017Z","iopub.status.idle":"2024-12-01T00:38:51.261642Z","shell.execute_reply.started":"2024-11-30T23:19:57.452993Z","shell.execute_reply":"2024-12-01T00:38:51.260706Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27da1aa774544882837e0549a341c5e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56a244d58f4a47c7a46665d0cd846280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6547799b6f8f4e75984aa733d9d53605"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2464' max='2464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2464/2464 1:17:56, Epoch 8/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Hazard Acc</th>\n      <th>Product Acc</th>\n      <th>Avg Accuracy</th>\n      <th>Avg F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>10.912800</td>\n      <td>10.866262</td>\n      <td>0.198538</td>\n      <td>0.019488</td>\n      <td>0.109013</td>\n      <td>0.050833</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>9.258100</td>\n      <td>9.066277</td>\n      <td>0.519285</td>\n      <td>0.058059</td>\n      <td>0.288672</td>\n      <td>0.221704</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>8.452900</td>\n      <td>8.275174</td>\n      <td>0.623224</td>\n      <td>0.117337</td>\n      <td>0.370280</td>\n      <td>0.313707</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>7.795900</td>\n      <td>7.799679</td>\n      <td>0.685749</td>\n      <td>0.142915</td>\n      <td>0.414332</td>\n      <td>0.360250</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>7.356100</td>\n      <td>7.472742</td>\n      <td>0.725538</td>\n      <td>0.168088</td>\n      <td>0.446813</td>\n      <td>0.397808</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>7.208300</td>\n      <td>7.259977</td>\n      <td>0.747056</td>\n      <td>0.182704</td>\n      <td>0.464880</td>\n      <td>0.415307</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>7.033800</td>\n      <td>7.130510</td>\n      <td>0.758019</td>\n      <td>0.189200</td>\n      <td>0.473609</td>\n      <td>0.426142</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>6.977200</td>\n      <td>7.088196</td>\n      <td>0.764921</td>\n      <td>0.190418</td>\n      <td>0.477670</td>\n      <td>0.431126</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [77/77 00:45]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for allenai/scibert_scivocab_uncased:\n{'eval_loss': 7.08819580078125, 'eval_hazard_acc': 0.7649208282582217, 'eval_product_acc': 0.1904181892001624, 'eval_avg_accuracy': 0.47766950872919206, 'eval_avg_f1': 0.43112616205141685, 'eval_runtime': 46.3307, 'eval_samples_per_second': 53.161, 'eval_steps_per_second': 1.662, 'epoch': 8.0}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T00:38:51.312775Z","iopub.execute_input":"2024-12-01T00:38:51.312982Z","iopub.status.idle":"2024-12-01T00:38:51.382005Z","shell.execute_reply.started":"2024-12-01T00:38:51.312959Z","shell.execute_reply":"2024-12-01T00:38:51.381167Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# concat_df = pd.read_csv('/kaggle/input/haz-prod-cat/submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T00:38:51.407297Z","iopub.execute_input":"2024-12-01T00:38:51.407537Z","iopub.status.idle":"2024-12-01T00:38:51.410892Z","shell.execute_reply.started":"2024-12-01T00:38:51.407514Z","shell.execute_reply":"2024-12-01T00:38:51.410090Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/working/final_cleaned_validation.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T07:24:12.312438Z","iopub.execute_input":"2024-12-01T07:24:12.313043Z","iopub.status.idle":"2024-12-01T07:24:12.338279Z","shell.execute_reply.started":"2024-12-01T07:24:12.313007Z","shell.execute_reply":"2024-12-01T07:24:12.337492Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# test_df = pd.concat([testdf, concat_df], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T00:38:51.437969Z","iopub.execute_input":"2024-12-01T00:38:51.438219Z","iopub.status.idle":"2024-12-01T00:38:51.441665Z","shell.execute_reply.started":"2024-12-01T00:38:51.438195Z","shell.execute_reply":"2024-12-01T00:38:51.440840Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T07:24:18.716322Z","iopub.execute_input":"2024-12-01T07:24:18.717020Z","iopub.status.idle":"2024-12-01T07:24:18.727884Z","shell.execute_reply.started":"2024-12-01T07:24:18.716989Z","shell.execute_reply":"2024-12-01T07:24:18.726961Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     year  month  day country  \\\n0    1994      5    5      us   \n1    1994      5   12      us   \n2    1995      4   16      us   \n3    1998      7   16      ca   \n4    1998      8    6      us   \n..    ...    ...  ...     ...   \n560  2022      6   29      au   \n561  2022      7   18      au   \n562  2022      7   20      ca   \n563  2022      7   28      hk   \n564  2022      7   28      us   \n\n                                                 title  \\\n0                     Recall Notification: FSIS-017-94   \n1                     Recall Notification: FSIS-048-94   \n2                     Recall Notification: FSIS-032-95   \n3    Archive - ALLERGY ALERT -- PRESENCE OF UNDECLA...   \n4                     Recall Notification: FSIS-018-98   \n..                                                 ...   \n560  The Fresh Salad Co Thai Coconut Wild Rice Prep...   \n561  Powered by Plants Pty Ltd — Cleanfit Plant Pro...   \n562  Certain Enjoy Life brand Soft Baked Cookies – ...   \n563    Imported biscuit may contain allergen (peanuts)   \n564  Wilbur’s of Maine Chocolate Confections Issues...   \n\n                                                  text  \n0    Date Opened: Date Closed: Name: KOEGEL MEATS I...  \n1    Date Opened: Date Closed: Name: COLUMBUS SALAM...  \n2    Date Opened: Date Closed: Recall Class: N Name...  \n3    PRESENCE OF UNDECLARED NUTS IN ORIGINALE AUGUS...  \n4    Recall Notification Report: RNR018-98 Date Ope...  \n..                                                 ...  \n560  Page Content ​ ​​​​ ​Date published: Product i...  \n561  PRA number 2022/19525 Published date Product d...  \n562  Food recall warning Certain Enjoy Life brand S...  \n563  Imported biscuit may contain allergen (peanuts...  \n564  Wilbur’s of Maine Chocolate Confections of Fre...  \n\n[565 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>country</th>\n      <th>title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1994</td>\n      <td>5</td>\n      <td>5</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-017-94</td>\n      <td>Date Opened: Date Closed: Name: KOEGEL MEATS I...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1994</td>\n      <td>5</td>\n      <td>12</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-048-94</td>\n      <td>Date Opened: Date Closed: Name: COLUMBUS SALAM...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1995</td>\n      <td>4</td>\n      <td>16</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-032-95</td>\n      <td>Date Opened: Date Closed: Recall Class: N Name...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1998</td>\n      <td>7</td>\n      <td>16</td>\n      <td>ca</td>\n      <td>Archive - ALLERGY ALERT -- PRESENCE OF UNDECLA...</td>\n      <td>PRESENCE OF UNDECLARED NUTS IN ORIGINALE AUGUS...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1998</td>\n      <td>8</td>\n      <td>6</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-018-98</td>\n      <td>Recall Notification Report: RNR018-98 Date Ope...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>560</th>\n      <td>2022</td>\n      <td>6</td>\n      <td>29</td>\n      <td>au</td>\n      <td>The Fresh Salad Co Thai Coconut Wild Rice Prep...</td>\n      <td>Page Content ​ ​​​​ ​Date published: Product i...</td>\n    </tr>\n    <tr>\n      <th>561</th>\n      <td>2022</td>\n      <td>7</td>\n      <td>18</td>\n      <td>au</td>\n      <td>Powered by Plants Pty Ltd — Cleanfit Plant Pro...</td>\n      <td>PRA number 2022/19525 Published date Product d...</td>\n    </tr>\n    <tr>\n      <th>562</th>\n      <td>2022</td>\n      <td>7</td>\n      <td>20</td>\n      <td>ca</td>\n      <td>Certain Enjoy Life brand Soft Baked Cookies – ...</td>\n      <td>Food recall warning Certain Enjoy Life brand S...</td>\n    </tr>\n    <tr>\n      <th>563</th>\n      <td>2022</td>\n      <td>7</td>\n      <td>28</td>\n      <td>hk</td>\n      <td>Imported biscuit may contain allergen (peanuts)</td>\n      <td>Imported biscuit may contain allergen (peanuts...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>2022</td>\n      <td>7</td>\n      <td>28</td>\n      <td>us</td>\n      <td>Wilbur’s of Maine Chocolate Confections Issues...</td>\n      <td>Wilbur’s of Maine Chocolate Confections of Fre...</td>\n    </tr>\n  </tbody>\n</table>\n<p>565 rows × 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test_df['text'] = test_df['text']\n# Drop 'hazard-category' and 'product-category'\n# test_df = test_df.drop(columns=['hazard-category', 'product-category'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T07:24:20.207152Z","iopub.execute_input":"2024-12-01T07:24:20.207488Z","iopub.status.idle":"2024-12-01T07:24:20.212300Z","shell.execute_reply.started":"2024-12-01T07:24:20.207461Z","shell.execute_reply":"2024-12-01T07:24:20.211324Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T07:24:21.460005Z","iopub.execute_input":"2024-12-01T07:24:21.460388Z","iopub.status.idle":"2024-12-01T07:24:21.473782Z","shell.execute_reply.started":"2024-12-01T07:24:21.460360Z","shell.execute_reply":"2024-12-01T07:24:21.472842Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"     year  month  day country  \\\n0    1994      5    5      us   \n1    1994      5   12      us   \n2    1995      4   16      us   \n3    1998      7   16      ca   \n4    1998      8    6      us   \n..    ...    ...  ...     ...   \n560  2022      6   29      au   \n561  2022      7   18      au   \n562  2022      7   20      ca   \n563  2022      7   28      hk   \n564  2022      7   28      us   \n\n                                                 title  \\\n0                     Recall Notification: FSIS-017-94   \n1                     Recall Notification: FSIS-048-94   \n2                     Recall Notification: FSIS-032-95   \n3    Archive - ALLERGY ALERT -- PRESENCE OF UNDECLA...   \n4                     Recall Notification: FSIS-018-98   \n..                                                 ...   \n560  The Fresh Salad Co Thai Coconut Wild Rice Prep...   \n561  Powered by Plants Pty Ltd — Cleanfit Plant Pro...   \n562  Certain Enjoy Life brand Soft Baked Cookies – ...   \n563    Imported biscuit may contain allergen (peanuts)   \n564  Wilbur’s of Maine Chocolate Confections Issues...   \n\n                                                  text  \n0    Date Opened: Date Closed: Name: KOEGEL MEATS I...  \n1    Date Opened: Date Closed: Name: COLUMBUS SALAM...  \n2    Date Opened: Date Closed: Recall Class: N Name...  \n3    PRESENCE OF UNDECLARED NUTS IN ORIGINALE AUGUS...  \n4    Recall Notification Report: RNR018-98 Date Ope...  \n..                                                 ...  \n560  Page Content ​ ​​​​ ​Date published: Product i...  \n561  PRA number 2022/19525 Published date Product d...  \n562  Food recall warning Certain Enjoy Life brand S...  \n563  Imported biscuit may contain allergen (peanuts...  \n564  Wilbur’s of Maine Chocolate Confections of Fre...  \n\n[565 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>country</th>\n      <th>title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1994</td>\n      <td>5</td>\n      <td>5</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-017-94</td>\n      <td>Date Opened: Date Closed: Name: KOEGEL MEATS I...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1994</td>\n      <td>5</td>\n      <td>12</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-048-94</td>\n      <td>Date Opened: Date Closed: Name: COLUMBUS SALAM...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1995</td>\n      <td>4</td>\n      <td>16</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-032-95</td>\n      <td>Date Opened: Date Closed: Recall Class: N Name...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1998</td>\n      <td>7</td>\n      <td>16</td>\n      <td>ca</td>\n      <td>Archive - ALLERGY ALERT -- PRESENCE OF UNDECLA...</td>\n      <td>PRESENCE OF UNDECLARED NUTS IN ORIGINALE AUGUS...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1998</td>\n      <td>8</td>\n      <td>6</td>\n      <td>us</td>\n      <td>Recall Notification: FSIS-018-98</td>\n      <td>Recall Notification Report: RNR018-98 Date Ope...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>560</th>\n      <td>2022</td>\n      <td>6</td>\n      <td>29</td>\n      <td>au</td>\n      <td>The Fresh Salad Co Thai Coconut Wild Rice Prep...</td>\n      <td>Page Content ​ ​​​​ ​Date published: Product i...</td>\n    </tr>\n    <tr>\n      <th>561</th>\n      <td>2022</td>\n      <td>7</td>\n      <td>18</td>\n      <td>au</td>\n      <td>Powered by Plants Pty Ltd — Cleanfit Plant Pro...</td>\n      <td>PRA number 2022/19525 Published date Product d...</td>\n    </tr>\n    <tr>\n      <th>562</th>\n      <td>2022</td>\n      <td>7</td>\n      <td>20</td>\n      <td>ca</td>\n      <td>Certain Enjoy Life brand Soft Baked Cookies – ...</td>\n      <td>Food recall warning Certain Enjoy Life brand S...</td>\n    </tr>\n    <tr>\n      <th>563</th>\n      <td>2022</td>\n      <td>7</td>\n      <td>28</td>\n      <td>hk</td>\n      <td>Imported biscuit may contain allergen (peanuts)</td>\n      <td>Imported biscuit may contain allergen (peanuts...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>2022</td>\n      <td>7</td>\n      <td>28</td>\n      <td>us</td>\n      <td>Wilbur’s of Maine Chocolate Confections Issues...</td>\n      <td>Wilbur’s of Maine Chocolate Confections of Fre...</td>\n    </tr>\n  </tbody>\n</table>\n<p>565 rows × 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"test_df = test_df[['text']]\ntest_texts = test_df['text'].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T07:24:21.853505Z","iopub.execute_input":"2024-12-01T07:24:21.854285Z","iopub.status.idle":"2024-12-01T07:24:21.862013Z","shell.execute_reply.started":"2024-12-01T07:24:21.854254Z","shell.execute_reply":"2024-12-01T07:24:21.861041Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nfrom safetensors.torch import load_file\nimport numpy as np\nimport torch\n\ndef get_model_logits(model_name, model_dir, test_texts, batch_size=8):\n    # Initialize the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Tokenize the test data\n    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n\n    # Convert tokenized inputs to a TensorDataset\n    test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'])\n\n    # Use DataLoader to load the data in batches\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n    # Define the number of labels\n    num_labels_dict = {\n        # 'hazard_category': num_hazard_category_labels,\n        # 'product_category': num_product_category_labels\n        'hazard': num_hazard_labels,\n        'product': num_product_labels\n    }\n\n    # Initialize the model\n    model = TransformerForFoodHazardClassification(model_name, num_labels_dict)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Load the model state dict from model.safetensors\n    if model_name == \"allenai/scibert_scivocab_uncased\":\n        state_dict = torch.load(f\"{model_dir}/scibert_weights\", map_location=device)\n        model.load_state_dict(state_dict)\n    else:\n        state_dict = load_file(f\"{model_dir}/model.safetensors\")\n        model.load_state_dict(state_dict)\n\n    # Move model to the GPUs (using DataParallel for multiple GPUs)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  # Default to cuda:0\n    model = torch.nn.DataParallel(model, device_ids=[0, 1])  # Use both GPU 0 and GPU 1\n    model.to(device)\n\n    # Initialize dictionaries to accumulate logits\n    # all_hazard_category_logits = []\n    # all_product_category_logits = []\n    all_hazard_logits = []\n    all_product_logits = []\n    \n    with torch.no_grad():\n        model.eval()\n        for batch in test_dataloader:\n            input_ids, attention_mask = [b.to(device) for b in batch]\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n            # Unpack logits and move them to CPU\n            hazard_logits, product_logits = outputs\n            # all_hazard_category_logits.append(hazard_category_logits.cpu().numpy())\n            # all_product_category_logits.append(product_category_logits.cpu().numpy())\n            all_hazard_logits.append(hazard_logits.cpu().numpy())\n            all_product_logits.append(product_logits.cpu().numpy())\n\n    # Concatenate all logits from batches along the batch dimension (axis=0)\n    # hazard_category_logits_concat = np.concatenate(all_hazard_category_logits, axis=0)\n    # product_category_logits_concat = np.concatenate(all_product_category_logits, axis=0)\n    hazard_logits_concat = np.concatenate(all_hazard_logits, axis=0)\n    product_logits_concat = np.concatenate(all_product_logits, axis=0)\n\n    # Free GPU memory by deleting model and test_encodings\n    del model\n    del test_encodings\n\n    # Clear CUDA cache\n    torch.cuda.empty_cache()\n\n    # Return logits as a structured dictionary\n    return {\n        # 'hazard_category': hazard_category_logits_concat,\n        # 'product_category': product_category_logits_concat\n        'hazard': hazard_logits_concat,\n        'product': product_logits_concat\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T07:24:22.440115Z","iopub.execute_input":"2024-12-01T07:24:22.440724Z","iopub.status.idle":"2024-12-01T07:24:22.450467Z","shell.execute_reply.started":"2024-12-01T07:24:22.440692Z","shell.execute_reply":"2024-12-01T07:24:22.449490Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# After evaluating the model\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:29:57.941064Z","iopub.execute_input":"2024-12-01T08:29:57.941429Z","iopub.status.idle":"2024-12-01T08:29:57.945625Z","shell.execute_reply.started":"2024-12-01T08:29:57.941399Z","shell.execute_reply":"2024-12-01T08:29:57.944721Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Get logits from DeBERTa Large\ndeberta_base_logits = get_model_logits('microsoft/deberta-base', 'deberta-base-model', test_texts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:30:18.825699Z","iopub.execute_input":"2024-12-01T08:30:18.826457Z","iopub.status.idle":"2024-12-01T08:30:40.300588Z","shell.execute_reply.started":"2024-12-01T08:30:18.826409Z","shell.execute_reply":"2024-12-01T08:30:40.299611Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# After evaluating the model\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:43:55.781196Z","iopub.execute_input":"2024-12-01T08:43:55.781558Z","iopub.status.idle":"2024-12-01T08:43:55.786271Z","shell.execute_reply.started":"2024-12-01T08:43:55.781521Z","shell.execute_reply":"2024-12-01T08:43:55.785405Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"distilbert_base_logits = get_model_logits('distilbert-base-uncased', 'distilbert-base-uncased-model', test_texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:43:55.990867Z","iopub.execute_input":"2024-12-01T08:43:55.991254Z","iopub.status.idle":"2024-12-01T08:44:02.962572Z","shell.execute_reply.started":"2024-12-01T08:43:55.991225Z","shell.execute_reply":"2024-12-01T08:44:02.961812Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# After evaluating the model\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:46:21.960916Z","iopub.execute_input":"2024-12-01T08:46:21.961556Z","iopub.status.idle":"2024-12-01T08:46:21.965637Z","shell.execute_reply.started":"2024-12-01T08:46:21.961513Z","shell.execute_reply":"2024-12-01T08:46:21.964635Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"scibert_logits = get_model_logits('allenai/scibert_scivocab_uncased', 'scibert_scivocab_uncased-model', test_texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:46:22.263679Z","iopub.execute_input":"2024-12-01T08:46:22.264002Z","iopub.status.idle":"2024-12-01T08:46:35.612028Z","shell.execute_reply.started":"2024-12-01T08:46:22.263974Z","shell.execute_reply":"2024-12-01T08:46:35.611143Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3824898770.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(f\"{model_dir}/scibert_weights\", map_location=device)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Average the logits\n# hazard_category_logits_avg = (distilbert_base_logits['hazard_category'] + deberta_base_logits['hazard_category']) / 2\n# product_category_logits_avg = (distilbert_base_logits['product_category'] + deberta_base_logits['product_category']) / 2\n# hazard_logits_avg = (distilbert_base_logits['hazard'] + deberta_base_logits['hazard'] + scibert_logits['hazard']) / 3\n# product_logits_avg = (distilbert_base_logits['product'] + deberta_base_logits['product'] + scibert_logits['product']) / 3\nhazard_logits_avg = scibert_logits['hazard']\nproduct_logits_avg = scibert_logits['product']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:46:37.215911Z","iopub.execute_input":"2024-12-01T08:46:37.216491Z","iopub.status.idle":"2024-12-01T08:46:37.220683Z","shell.execute_reply.started":"2024-12-01T08:46:37.216456Z","shell.execute_reply":"2024-12-01T08:46:37.219947Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Get predicted labels\n# hazard_category_preds = np.argmax(hazard_category_logits_avg, axis=1)\n# product_category_preds = np.argmax(product_category_logits_avg, axis=1)\nhazard_preds = np.argmax(hazard_logits_avg, axis=1)\nproduct_preds = np.argmax(product_logits_avg, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:46:38.544665Z","iopub.execute_input":"2024-12-01T08:46:38.545793Z","iopub.status.idle":"2024-12-01T08:46:38.550722Z","shell.execute_reply.started":"2024-12-01T08:46:38.545744Z","shell.execute_reply":"2024-12-01T08:46:38.549824Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Decode the predicted labels using the label encoders\n# hazard_category_labels = hazard_category_encoder.inverse_transform(hazard_category_preds)\n# product_category_labels = product_category_encoder.inverse_transform(product_category_preds)\nhazard_labels = hazard_encoder.inverse_transform(hazard_preds)\nproduct_labels = product_encoder.inverse_transform(product_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:46:40.139498Z","iopub.execute_input":"2024-12-01T08:46:40.139828Z","iopub.status.idle":"2024-12-01T08:46:40.145500Z","shell.execute_reply.started":"2024-12-01T08:46:40.139799Z","shell.execute_reply":"2024-12-01T08:46:40.144506Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Create a DataFrame for the predictions\noutput_df = pd.DataFrame({\n    'hazard': hazard_labels,\n    'product': product_labels\n})\n\n# Save the output DataFrame to a CSV file\noutput_df.to_csv('test_predictions_ensemble.csv', index=False)\n\n# For subtask 1 (hazard-category and product-category)\nsubtask2_df = output_df[['hazard', 'product']]\nsubtask2_df.to_csv('subtask2_predictions_ensemble.csv', index=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:46:40.882720Z","iopub.execute_input":"2024-12-01T08:46:40.882974Z","iopub.status.idle":"2024-12-01T08:46:40.892273Z","shell.execute_reply.started":"2024-12-01T08:46:40.882950Z","shell.execute_reply":"2024-12-01T08:46:40.891553Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# Analyze the predictions\nprint(\"Hazard Predictions:\")\nprint(subtask2_df['hazard'].value_counts())\n\nprint(\"\\nProduct Predictions:\")\nprint(subtask2_df['product'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T08:46:41.587738Z","iopub.execute_input":"2024-12-01T08:46:41.588003Z","iopub.status.idle":"2024-12-01T08:46:41.595545Z","shell.execute_reply.started":"2024-12-01T08:46:41.587977Z","shell.execute_reply":"2024-12-01T08:46:41.594526Z"}},"outputs":[{"name":"stdout","text":"Hazard Predictions:\nhazard\nsalmonella                                        78\nlisteria monocytogenes                            75\nmilk and products thereof                         72\nother                                             49\nescherichia coli                                  30\ninspection issues                                 30\npeanuts and products thereof                      29\ncereals containing gluten and products thereof    26\nsoybeans and products thereof                     24\neggs and products thereof                         24\nplastic fragment                                  20\nmetal fragment                                    16\nsulphur dioxide and sulphites                     13\nglass fragment                                    12\nalmond                                            10\nsesame seeds and products thereof                  7\nunauthorised substance ethylene oxide              6\nmustard and products thereof                       5\nclostridium botulinum                              5\nmoulds                                             4\nwalnut                                             3\nheavy metals                                       3\nsulphur dioxide                                    2\nprocessing                                         2\ncashew                                             2\ninsects                                            2\nnorovirus                                          2\nbacillus spp.                                      2\nenteroviruses                                      1\nAflatoxin                                          1\npolycyclic aromatic hydrocarbons                   1\nfish and products thereof                          1\nundeclared additive                                1\ne 425 - konjac unauthorised                        1\ntoxin                                              1\npatulin                                            1\ncrustaceans and products thereof                   1\ncyclospora                                         1\npecan nut                                          1\ntoo high content of tetrahydrocannabinol (THC)     1\nName: count, dtype: int64\n\nProduct Predictions:\nproduct\nchicken based products            159\nice cream                         126\ncheese                             61\ncakes                              29\nground beef                        25\ndietary supplement                 22\npeanuts                            21\nready to eat - cook meals          20\ncookies                            11\nbeer                                8\ndried apricots                      8\nfresh pork                          6\nsultanas                            6\noysters                             5\nbiscuits                            5\njellies                             5\npet feed                            4\nsalmon                              4\nfeed materials                      4\nmilk                                4\nice sticks                          2\nfood supplement                     2\nthermal processed pork meat         2\nluncheon meat                       2\ncurry                               2\ncandies                             2\nvarious vegetable preparations      2\nginger powder                       1\nsoy sauce                           1\nolive oil                           1\nbeef stewed                         1\nchilled dumplings with meat         1\nflaxseed                            1\nbuffalo milk mozzarella cheese      1\nbottled water                       1\nall purpose seasoning               1\napple juice                         1\npickled radish                      1\nsalads                              1\nseaweed preparations                1\nspice marinade                      1\npistachio nuts                      1\ninfant formula                      1\ncbd oil                             1\npear juice                          1\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}